{
  "data": [
    {
      "id": 9,
      "attributes": {
        "Heading": "Naive Bayes Classifier",
        "createdAt": "2022-03-06T12:57:02.943Z",
        "updatedAt": "2022-03-06T13:04:10.966Z",
        "publishedAt": "2022-03-06T12:57:04.202Z",
        "PublishedDate": "2022-02-24",
        "ContentBody": "<div class=\"mt-6 ml-0 max-w-none md:ml-10 prose xl:prose-lg select-none\"><p>In Machine Learning and Data Science field, researchers have developed many advanced algorithms like Support Vector Machines, Logistic Regression, Gradient Boosting, etc. These algorithms are capable enough to produce very high accuracy. But among these advanced ones, there exists an elementary and Naive algorithm, known as Naive Bayes.&nbsp;</p>\n<p>In English, the \"Naive\" word is used for a person or action lacking experience, wisdom, or judgment. This tag is associated with the Naive Bayes algorithm because it also makes some silly assumptions while making any predictions. But the most exciting thing is it still performs better or equivalent to the best algorithms. So let's learn about this algorithm in greater detail.</p>\n<h2>Key takeaways from this&nbsp;blog</h2>\n<p>After going through this blog, we will have an understanding of the following things:</p>\n<ol>\n<li>What is Bayes theorem?</li>\n<li>Why Bayes theorem in Machine Learning?</li>\n<li>Naive Bayes examples for single and multiple features.</li>\n<li>How does Naive Bayes handle the non-categorical features?</li>\n<li>What is Gaussian Naive Bayes?</li>\n<li>Python-based implementation</li>\n<li>Advantages and disadvantages of Naive Bayes.</li>\n<li>Industrial applications of Naive Bayes.</li>\n<li>Possible Interview Questions on this topic.</li>\n</ol>\n<p>Let's start without any further delay.</p>\n<h2>The Basic Intuition of Baye's&nbsp;Theorem</h2>\n<p>In our <a href=\"https://www.enjoymathematics.com/blog/probability-theory-for-machine-learning\">probability blog</a>, we discussed Baye's theorem. Let's use the same example here.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*Vql4mVP0Q7R61Au4Nl-_2Q.jpeg\" alt=\"Probability Bayes theorem basics, source: ifunny.co\" title=\"Probability Bayes theorem basics \"></p>\n<h4>Terms in the above example can be read like&nbsp;this:</h4>\n<p>P(chill | Netflix)&nbsp;:= Probability that we enjoy if we are watching Netflix.</p>\n<p>P(Netflix | chill)&nbsp;:=Probability that we are watching Netflix if we know that we are chilling/enjoying.</p>\n<p>P(chill)&nbsp;:= Probability that we are chilling/enjoying.</p>\n<p>P(Netflix)&nbsp;:= Probability that we are watching Netflix.</p>\n<p>So according to Bayes Theorem, if we want to know the probability that we are enjoying if we are watching Netflix (i.e., P(chill | Netflix)), we must know two things:</p>\n<ul>\n<li><strong>Likelihood probabilities:</strong> The probability that we are watching Netflix if we know that we are chilling/enjoying, i.e., P(Netflix | chill)</li>\n<li><strong>Prior probabilities:</strong> Probabilities that we are enjoying and the Probability that we are watching Netflix. P(chill) and P(Netflix).</li>\n</ul>\n<h2>Why Bayes Theorem in Machine Learning?</h2>\n<p>Let's try to find the answer to this question. In the case of supervised learning, we have input features and the corresponding output labels present with us. We try to make our machines learn the relationship between input features to the output variable. Once this learning is done on training data, we can use this model to make predictions on test data. Let's represent this supervised approach in a Bayesian format.&nbsp;</p>\n<p>We know the feature values for test data and want to see the output label if that particular values of features are present. We can also say we want to predict the chances of occurrence of any label if the values of features are already known. That is precisely the same as <strong>p(label | feature).</strong></p>\n<p>From the Bayes theorem, to know the value of p(label | feature), we must know likelihood probabilities p(feature | label) and the prior probabilities, p(label) and p(features). <strong>But, do we really have these values?</strong></p>\n<p>Yes! from the training data. That's the whole crux of supervised learning. Right?</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*EIVtLAe05L4htlZeMc8OFA.jpeg\" alt=\"baye's theorem terms \" title=\"baye's theorem terms \"></p>\n<p>Let's discuss the terms on the right side in the formulae above. The likelihood term p(feature | label) says the probability of that feature if we already know the label. And for training data, we know the label for each sample. Also, the prior probabilities p(label) and p(feature) can be calculated from the training data. So ultimately, we will have the posterior probability that we wanted to calculate.</p>\n<h2>Naive Bayes&nbsp;example</h2>\n<h3>Single Feature</h3>\n<p>Let's take the example of a football game, and below is the data which says if humidity is high or normal, then play happens; otherwise, if humidity is low, the play does not occur. Straightforward data, correct?</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*dEgAxCZ456u4TTUYlDrwAQ.png\" alt=\"Data snippet\" title=\"Data snippet\"></p>\n<p>Suppose we want to make a machine learning model, which receives the feature value of humidity and tries to predict whether the play will happen or not. So suppose we know the humidity as Normal, and let's calculate the chances of play to happen, i.e., p(play = Yes | humidity = Normal),</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*PK1H1tjDObbWJHw-Z_QTow.png\" alt=\"Estimating posetrior probability \" title=\"Estimating posetrior probability \"></p>\n<p>From the data,&nbsp;</p>\n<p><strong>p(humidity = Normal | play = Yes)</strong>&nbsp;:= Probability of humidity to be normal when we know that play happened is 1/2, as there are two cases when the play happened. One is when humidity = High and Humidity = Normal. So the probability of humidity = Normal will be 0.5.</p>\n<p><strong>p(play = Yes)&nbsp;:=</strong> Probability of game to have happened will be 2/3 as we have 3 samples out of which 2 says the play happened as \"Yes\".</p>\n<p><strong>p(humidity = Normal)&nbsp;:=</strong> probability of humidity to be normal will be 1/3 as there are three samples out of which 1 instance is of humidity = Normal.</p>\n<p>So,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*3AKJx9uS4hZ2dwQJag27Xg.png\" alt=\"Estimating posetrior probability 2\" title=\"Estimating posetrior probability 2\"></p>\n<p>The same can be understood from the data intuition as well. Similarly, p(play= No | humidity = Normal) can be calculated, and it will be 0.</p>\n<h3>Multiple Feature</h3>\n<p>The example we saw above had just one feature, the humidity value. But in the practical, real-life data, we will have multiple features. In such a case, the same equation can be represented as,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*7EDIZA-m1M5_pVgEbJ4C7Q.png\" alt=\" posetrior probability for multiple features\" title=\" posetrior probability for multiple features\"></p>\n<p>Expanding the above equation,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*qXHGflbJly7-NQ_0cuc4HA.png\" alt=\" posetrior probability for multiple features 2\" title=\" posetrior probability for multiple features 2\"></p>\n<p>This calculation is extensive and expansive in terms of computations. That's where the Naïve Bayes algorithms come with their \"naive\" assumption. It considers every feature is independent of each other, which means one feature is unaffected by the occurrence of any other feature.&nbsp;</p>\n<p>This assumption is impractical in real-life scenarios. Suppose we are recording features from our cell phone's battery, the terminal voltage, current, and temperature. Using these, we estimate whether battery health is \"good\" or \"bad\". It's a classification problem statement, and algorithms like SVM, logistic regression, etc., will learn mapping functions between feature sets and labels. While learning, these algorithms will not assume that current is independent of the Voltage feature, which is oblivious. Voltage produces the current, which results in the heating of the battery, so temperature varies accordingly. So all features are dependent.&nbsp;</p>\n<p>But Naive Bayes algorithm assumes that all three features are independent, and the occurrence of one feature is totally unaffected by the occurrence of others. This is not true in practical cases. Still, this algorithm produces quite fascinating results. According to Naïve Bayes, the above equation is modified as follows:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*fBQzL_cQ_RvJTV9aJcwiEA.png\" alt=\" posetrior probability for multiple features 3\" title=\" posetrior probability for multiple features 3\"></p>\n<p>This assumption drastically reduces the computation cost and also delivers good accuracy. Also, in the Naive Bayes algorithm, we don't calculate the denominator of the above equation to save some computation power. Because, for all the classes, the denominator term remains the same and does not make any contribution in segregating different classes. Hence, we can represent the above equation as:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*OA21o89W2WGfh61CItKMdA.png\" alt=\" posetrior probability for multiple features 4\" title=\" posetrior probability for multiple features 4\"></p>\n<h2>How does Naive Bayes handle the non-categorical features?</h2>\n<p>What if the features are continuous numerical values (non-categorical). How will we estimate probabilities in such a case?&nbsp;</p>\n<p>Here comes the theory of <a href=\"https://www.enjoymathematics.com/blog/probability-distribution-function-for-machine-learning\">Probability Distribution Function</a> (PDF). We need to estimate the probabilities for numerical variables using this concept.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*k_LvuyKjbN1Gr54aZ06GGw.png\" alt=\"Gaussian distribution continuous vs discrete\" title=\"Gaussian distribution continuous vs discrete\"></p>\n<p>Suppose we assume that the PDF is a gaussian or normal distribution. In that case, we need to calculate the <strong>mean(μ)</strong> and <strong>standard deviation (σ)</strong> of that feature,and then for any value of feature x, the probability <strong>f(x)</strong> can be calculated from the below equation.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*eC6_xSUmSbshBxsCtpl-pQ.png\" alt=\"Normal Distribution formulae\" title=\"Normal Distribution formulae\"></p>\n<h2><strong>Gaussian Naïve&nbsp;Bayes</strong></h2>\n<p>In the case of gaussian distribution assumption, we call Naïve Bayes, <strong>Gaussian Naïve Bayes</strong> algorithm. We can choose other probability distribution functions, like Bernoulli, Binomial, etc., to estimate probabilities. Among all these, gaussian is the most famous one as most of the real-world sensors produce data in the gaussian distribution format. If we look at the image below closely, most people in the gym use 15–20 Kg weights. Later on, the usage of other weights decreases subsequently, like what we observe in the gaussian distribution.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*8OeMInFFHZ4_NrO9uAOq_w.jpeg\" alt=\"Real-life normal distribution\" title=\"Real-life normal distribution\"></p>\n<p>Too much theory, let's implement it on a practical machine learning application, i.e., Sentiment analysis.</p>\n<h2>Can we solve regression problems using Naive&nbsp;Bayes?</h2>\n<p>Although the native form of the Naive Bayes algorithm is made to solve the classification problems only, a team tried to solve the regression problem using Naive Bayes, and the work can be found <a href=\"https://link.springer.com/content/pdf/10.1023%2FA%3A1007670802811.pdf\">here</a>. They concluded that they did not achieve good results and suggested that Naive Bayes is suitable for classification algorithms.</p>\n<h2>Naive Bayes Sklearn In&nbsp;Python</h2>\n<p>Let's train a Naive Bayes algorithm on the famous Iris dataset. The objective of our algorithm would be to look onto the available features like Sepal/Petal length/width and classify flowers into three categories of Setosa, Versicolor, and Virginica.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*na4VvO08G-ec0RDulgBOLQ.jpeg\" alt=\"Iris data visualization\" title=\"Iris data visualization\"></p>\n<h3>Step 1: Importing required libraries</h3>\n<p>Standard libraries like numpy (for numeric operations), pandas (for managing the data), and matplotlib (to visualize the analysis of dataset) would be required.</p>\n<pre><code class=\"language-python\">import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</code></pre>\n<h3>Step 2: Loading the dataset and visualize scatter-plot of&nbsp;features</h3>\n<p>The famous iris dataset comes with the Scikit-learn library. If we print the features present in the dataset, the output is ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']. Also, the target variable y can take three values [0, 1, 2] corresponding to three flower classes.</p>\n<pre><code class=\"language-python\">from sklearn import datasets\n\niris = datasets.load_iris()            #loading dataset\n\nX = iris.data[:,]                    #input\ny = iris.target                      #target\n\nprint(\"Features : \", iris['feature_names'])\n\niris_dataframe = pd.DataFrame(data=np.c_[iris['data'],iris['target']],\n                             columns=iris['feature_names']+['target'])\n\nplt.figure()\ngrr = pd.plotting.scatter_matrix(iris_dataframe, c=iris['target'],\n                                  figsize=(15,5),\n                                  s=60,alpha=8)\nplt.show()\n\n# Features :  ['sepal length (cm)', 'sepal width (cm)', 'petal      # length (cm)', 'petal width (cm)']\n</code></pre>\n<p>The code above will plot the diagram shown below.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*FMtANlM6hRH7-NCJqfaiHg.jpeg\" alt=\"IRIS Data pair plot\" title=\"IRIS Data pair plot\"></p>\n<h3>Step 3: Visualize the correlation and check the assumption of Naive&nbsp;Bayes</h3>\n<p>We can plot the correlation matrix using the seaborn library.</p>\n<pre><code class=\"language-python\">import seaborn as sns\ndataplot = sns.heatmap(iris_dataframe.corr(), annot=True)\n\nplt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*GLRwmL4iQoRITJRZCNIzog.jpeg\" alt=\"Cross correlation between features\" title=\"Cross correlation between features\"></p>\n<p>We can see that the features are highly correlated. But as per Naive Bayes assumption, it will treat features as entirely independent to each other. Based on this, our algorithm will compute the following probability for all three flower classes.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*f6w3BLQiF_AV_85l7lHXUg.png\" alt=\"Naive bayes for iris data\" title=\"Naive bayes for iris data\"></p>\n<h3>Step 4: Split the&nbsp;dataset</h3>\n<p>Now we are in a position where we can segregate our training and testing data. For that, we can use the inbuilt function of train<em>test</em>split from Sklearn.</p>\n<pre><code class=\"language-python\">from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n</code></pre>\n<h3>Step 5: Fit the&nbsp;model</h3>\n<p>Now, assume our data follows Gaussian distribution and import the Gaussian Naive Bayes <strong>GaussianNB</strong> model from Sklearn. Let's fit this model into our training data.</p>\n<pre><code class=\"language-python\">from sklearn.naive_bayes import GaussianNB\n\nNB = GaussianNB()\nNB.fit(X_train, y_train)\n</code></pre>\n<p>Hurrah! We have our model ready with us now.</p>\n<h3><strong>Step 6: Evaluate the&nbsp;model</strong></h3>\n<p>We must know that we have solved a classification problem to evaluate the model. Hence some standard metrics for evaluating our model can be accuracy, precision, recall, F-1 Score, etc. A detailed list can be found here in this <a href=\"https://www.enjoyalgorithms.com/blog/evaluation-metrics-classification-models\">blog</a>. As we know, all these matrices can be calculated by having the confusion matrix ready with us. So let's plot the confusion matrix.</p>\n<pre><code class=\"language-python\">Y_pred = NB.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, Y_pred)\n\ndf_cm = pd.DataFrame(cm, columns=np.unique(y_test), index = np.unique(y_test))\n\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\n\nsns.heatmap(df_cm, annot=True)# font size\nplt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*TzjVqa6_8gfiLlushOJw_g.jpeg\" alt=\"Confusion matrix for naive bayes\" title=\"Confusion matrix for naive bayes\"></p>\n<p>From the matrix, we can see that the model quickly achieved 100% accuracy on test data. This is really amazing. It started with the silly assumption but provided whatever we wanted. That's the main reason why this algorithm is famous among researchers.</p>\n<h2>Advantages of Naive&nbsp;Bayes</h2>\n<ol>\n<li><strong>Computationally Simple:</strong> This classifier is computationally very simple compared to algorithms like SVM and XGBoost.&nbsp;</li>\n<li>When the independent nature of features becomes true in data, the Naive Bayes algorithm performs the best and can beat the accuracy of <a href=\"https://www.enjoyalgorithms.com/blog/logistic-regression-in-ml\">Logistic Regression</a>.</li>\n<li><strong>Multi-class prediction:</strong> It can provide us the probabilities do different classes and hence can give multi-class predictions.</li>\n<li>It works best in the case of Categorical variables compared to the numerical ones.</li>\n</ol>\n<h2>Disadvantages of Naive&nbsp;Bayes</h2>\n<ol>\n<li><strong>Independent treatment of features:</strong> One of the most significant disadvantages of this algorithm is that it treats every feature independently, which can be cumbersome most of the time. Advanced classifiers like SVMs learn the relationship between the feature and target variables and the relationship among features. This is not the case with Naive Bayes.</li>\n<li><strong>Zero frequency case:</strong> Probably, some values from a categorical variable do not appear in training data. In such a case, Naive Bayes will assign a zero probability to these samples when they appear in test data. This is called \"Zero Frequency\" and can be cured using smoothing techniques, like <a href=\"https://en.wikipedia.org/wiki/Additive_smoothing\">Laplace estimation</a>.</li>\n<li><strong>Bad Estimator:</strong> Naive Bayes is considered to be a lousy probability estimator, so we should not take the probability predicted by this algorithm too seriously.&nbsp;</li>\n</ol>\n<h2>Industrial Applications of Naive&nbsp;Bayes</h2>\n<p>This algorithm is naive, but because of its performance, several industrial applications are based on this.</p>\n<ol>\n<li><strong>Recommendation System:</strong> <a href=\"https://www.enjoyalgorithms.com/blog/recommender-system-using-machine-learning\">Collaborative Filtering</a> and the Naive Bayes algorithm form the recommendation system that can recommend some products/movies/songs to users.</li>\n<li><strong>Multi-class prediction:</strong> One of the most significant advantages of Naive Bayes is its applicability in predicting the probability of multiple classes.&nbsp;</li>\n<li><strong>Real-time prediction:</strong> Because of its lesser complex nature, it can give prediction results very fast. Many advanced algorithms like SVM XGBoost are too heavy sometimes for smaller classification tasks. In such a scenario, Naive Bayes performs well.</li>\n</ol>\n<h2>Possible Interview Question</h2>\n<p>The theory of this algorithm is really important as it is rare to find full-scale projects in resumes related to this algorithm. Interviewers want to test the probability knowledge from this topic. Some popular questions can be:</p>\n<ol>\n<li>Explain the Bayes Theorem and associate it with the Naive Bayes algorithm.</li>\n<li>Why is this algorithm called Naive?</li>\n<li>How do these probability calculations hold for continuous numerical variables?</li>\n<li>What is Gaussian Naive Bayes algorithm? What variants can be possible or why Gaussian?</li>\n</ol>\n<h2>Conclusion</h2>\n<p>In this article, we learned about the Naive Bayes classifier in detail. We drew the intuition from Baye's theorem of probability and stated its correlation with the Naive Bayes algorithm. We also saw the python implementation of this algorithm using sklearn. In the last, we discussed the advantages, disadvantages, and industrial applications. We hope you enjoyed the article.</p></div>",
        "Description": "Naive Bayes is a popular supervised machine learning algorithm used to predict the categorical target variables",
        "AuthorAndReviewer": "Ravish Raj",
        "Tag": "MachineLearning",
        "DomainFlagTag": "MachineLearning",
        "coverImage": "https://www.enjoyalgorithms.com/static/naive-bayes-in-ml-cover-b420fc116f43328b9e0d04fe98f91646.jpg"
      }
    },
    {
      "id": 10,
      "attributes": {
        "Heading": "K-Nearest Neighbors (KNN)",
        "createdAt": "2022-03-07T05:04:32.552Z",
        "updatedAt": "2022-03-07T05:04:34.750Z",
        "publishedAt": "2022-03-07T05:04:34.748Z",
        "PublishedDate": "2022-02-20",
        "ContentBody": "<div class=\"mt-6 ml-0 max-w-none md:ml-10 prose xl:prose-lg select-none\"><p>K-Nearest Neighbor is a Supervised Machine Learning algorithm that can be used to solve classification as well as regression problems. It is probably the first \"machine learning\" algorithm developed, and because of its simple nature, it is still widely accepted in solving many industrial problems. The unique thing about this algorithm is it learns but without explicitly mapping input variables to the target variables. In this article, we are going to understand this algorithm in detail.</p>\n<h2>Key Takeaways from this&nbsp;blog:</h2>\n<p>After going through this article, we will understand the following things:</p>\n<ol>\n<li>What is the KNN algorithm in Machine Learning?</li>\n<li>Why is KNN instance-based learning or a Lazy learner?</li>\n<li>Why KNN is a non-parametric algorithm?</li>\n<li>What are the common assumptions in KNN?</li>\n<li>How does KNN work?</li>\n<li>How the value of K affects the KNN algorithm?</li>\n<li>How does feature scaling affect KNN?</li>\n<li>What are the Voronoi cell and Voronoi diagrams?</li>\n<li>KNN for regression problems.</li>\n<li>Implementation of the KNN algorithm in python.</li>\n</ol>\n<p>So let's start without any further delay.</p>\n<h2>What is the KNN algorithm in Machine Learning?</h2>\n<p>In the introduction section, we already have explained KNN formally. Now, let's understand it in layman's terms. Some friends did not understand the concepts in our school days and still scored well in exams because of their memorization skills. We can correlate those friends with KNN. This ML algorithm does not follow the traditional approach of learning parameters from the training data and tries to fit a function. Instead, it memorizes the complete training data instances, and whenever a new test sample comes, it tries to verify the similarity of the test sample with its learned training samples.</p>\n<h2>Why is KNN instance-based learning or a Lazy&nbsp;learner?</h2>\n<p>Instance-based learning is also known as memory-based learning<strong>.</strong> Instead of explicit generalization, KNN compares new data samples with training data samples present in its memory.</p>\n<p>They are also called lazy algorithms, as any computation only happens when we receive new observations. Before accepting any test sample, it just memorizes everything in its memory and defers the calculations for the last like a lazy person.</p>\n<h2>Why KNN is a non-parametric algorithm?</h2>\n<p>KNN comes under the <strong><a href=\"https://www.enjoyalgorithms.com/blog/classification-of-machine-learning-models\">non-parametric algorithm</a></strong> category. Can we guess why? It is learning the complete training set, so if there are more instances in the future, the learning will change drastically. Hence learning is not dependent on the given data, which is a characteristic of a non-parametric algorithm.</p>\n<h2>What are the common assumptions in&nbsp;KNN?</h2>\n<p>This algorithm makes two major assumptions,</p>\n<ul>\n<li>Every sample part of the training data is mapped to real n-dimensional space. We can say that every sample will have the same dimension or number of attributes in simple terms.&nbsp;</li>\n<li>The \"nearest neighbors\" are defined in terms of <strong>Euclidean Distance</strong>, <strong>Manhattan Distance</strong>, or <strong>Hamming Distance</strong>. The choice of distance matters a lot and can change the prediction.</li>\n</ul>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/0*MBTyUQ648MfodIMb.png\" alt=\"Distance metrics\" title=\"Distance metrics\"></p>\n<h2><strong>Working of&nbsp;KNN</strong></h2>\n<p>Let's understand the stepwise analysis of this algorithm for any classification problem.</p>\n<p><strong>Step1:</strong> We first need to select the number of neighbors we want to consider. This is the term K in the KNN algorithm and highly affects the prediction.</p>\n<p><strong>Step2: We</strong> need to find the K neighbors based on any distance metric. It can be Euclidean, Manhatten, or our custom distance metric. We will have the test sample on which we want the prediction. The Closest K samples in the training data from this test sample will be our K neighbors.</p>\n<p><strong>Step3:</strong> Among the selected K neighbors, we need to count how many neighbors are from the different classes.&nbsp;</p>\n<p><strong>Step4:</strong> Now, we have to assign the test data sample to the class for which the count of neighbors was maximum.</p>\n<p>We performed the prediction in these four simple steps. In summary, the KNN algorithm at the training phase stores the dataset, and when it gets a new query, it classifies that query into a class similar to the existing query.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*-S8Zf-MufUStk1OEyEzuFQ.jpeg\" alt=\"1-NN vs. 4-NN \" title=\"1-NN vs. 4-NN\"></p>\n<p>Consider an example shown in the above image. Initially, the entire training dataset is considered and mapped in an R² space of positive and negative classes. The test case <strong>xq</strong> is then classified using 1-NN (1 neighbor) and 4-NN (4 neighbors) classifiers. The results for both are different, as we see that <strong>xq</strong> is classified as <strong>+ve for 1-NN</strong> and <strong>-ve for 4-NN.</strong>&nbsp;</p>\n<h2>How the value of K affects the KNN algorithm?</h2>\n<p>The value of K in the KNN algorithm can be anything ranging from 1 to the total number of samples. A small value of K means that the model is overfitting and is vulnerable to outliers. This model will have high variance and low bias. On the other hand, a model with a high value of K will have low variance and high bias and will result in underfitting. When we slowly increase the value of K from 1 to the number of training samples, the model will start smoothing the boundary surfaces.</p>\n<p><strong>K = 1:</strong> A model with K=1 will have 0 training error and hard boundaries for determining the class of test query.</p>\n<p><strong>K = len(sample data):</strong> This model will be highly biased towards the majority class (with a higher number of samples) and less accurate.</p>\n<p><strong>Note:</strong> Keeping the K values as odd is advisable to reduce the chances of getting a tie.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*7yP2rK3WSNvw_rSXecvPnQ.jpeg\"></p>\n<h2>How does feature scaling affect&nbsp;KNN?</h2>\n<p>KNN depends highly on the distance between data samples; hence scaling plays a vital role here. Suppose we train the KNN algorithm on unscaled data. There can be a case where different attributes lie in various scales, making our model biased towards the features with lesser magnitude values. To avoid that, it is always advisable to standardize the attributes before applying the KNN algorithm. Please look at <a href=\"https://www.enjoyalgorithms.com/blog/need-of-feature-scaling-in-machine-learning\">this blog</a> to visualize how distance calculation can be affected by scaling for a better understanding.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/0*-tv0vjTEud6hDKWn.png\" alt=\"Source: Scikit-learn.org, Scaling affect KNN \" title=\"Scaling affect KNN \"></p>\n<h2>What are the Voronoi cell and Voronoi diagrams?</h2>\n<p>Other ML algorithms like linear regression, logistic regression, and SVMs try to fit a mapping function from input to output. This mapping function is also known as the <strong>Hypothesis function</strong>. But, KNN is different. It does not form any explicit Hypothesis function, but it does create a hypothesis space. For a dataset in R², the hypothesis space is a polyhedron formed using the training samples. Let's first understand what Voronoi cell is.</p>\n<h3><strong>What is Voronoi&nbsp;Cell?</strong></h3>\n<p>Suppose the training set is \"T\" and the elements of that training set are \"x\"<strong>.</strong> Then Voronoi Cell of <strong>xi</strong> is a polytope (a geometric shape with \"flat\" sides) consisting of all points closer to <strong>xi</strong> than any other points in <strong>T.</strong></p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*vvDVXsv2ulT_-IaikeAXNg.jpeg\" alt=\"Voronoi Cell and polytope\" title=\"Voronoi Cell and polytope\"></p>\n<p>If we observe in the above image, initially, every cell contains a single sample which means K = 0, and as we increase the value of K, two cells merge and form a new polytope including K samples. Voronoi Cells cover the entire training space of T, and when we combine all of these cells, it will create Voronoi Diagram.</p>\n<h2>KNN for Regression problems</h2>\n<p>So far, we have discussed how we could use the KNN algorithm to solve the classification tasks, but this machine learning algorithm can also solve regression problems. We need to tweak the approach slightly. Instead of counting the <strong>K</strong> nearest neighbor class labels, what if we average the data over K neighbors<strong><em>.</em></strong> Yes! It will act as the regression model in such a scenario.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*RWeDQNYq_K3bLqA6K3ni1A.jpeg\">k-NN for Regression tasks</p>\n<p>For example, let's say we have a test data X for which we want to predict the continuous variable Y. Suppose we have finalized that our neighbors can only be 3 (i.e., K=3). Three neighbors from the training data are:</p>\n<p>X1 → Y1, X2 → Y2, X3 → Y3. We should be clear that KNN is a supervised learning algorithm, and hence we will always have the corresponding labels for the input variables while training. At the time of prediction, we can average out the three labels to find the corresponding label of the test data. For example, Y = (Y1 + Y2 + Y3)/3. This averaging can be replaced with other techniques like median, mode, or any custom approach.&nbsp;</p>\n<h2>Strengths of KNN algorithm</h2>\n<p>KNN is a very famous algorithm because of its simplicity, so let's understand the key strengths.&nbsp;</p>\n<ol>\n<li><strong>Zero training time:</strong> A very little training time is required compared to the other machine learning algorithms.</li>\n<li><strong>Sample efficiency:</strong> There is no need for a very high training sample.</li>\n<li><strong>Explainable:</strong> At each step, the reason for the prediction can easily be depicted. Such explainability is rare.</li>\n<li><strong>Easy to add and remove the data:</strong> For other machine learning models, data addition requires retraining of the model. While in KNN, we can directly update the memory and perform the inference.</li>\n<li><strong>Less sensitive to class imbalance:</strong> Suppose we have two classes and one class has significantly higher instances in the dataset than others. KNN, unlike other ML algorithms, is least affected by such class imbalances.</li>\n</ol>\n<h2>Disadvantages of k-NN algorithm</h2>\n<p>No doubt, KNN is cool, but this algorithm has some limitations. It is not the first choice among Machine Learning experts, and the reasons are:</p>\n<ol>\n<li><strong>Needs a lot of storage:</strong> KNN stores the whole training data in its memory and performs inference based on that. It makes the algorithm unemployable on edge platforms.</li>\n<li><strong>Predictions are Slow:</strong> The time complexity of KNN is O(dN), where <strong>d</strong> is the dimension or number of features and <strong>N</strong> is the total number of samples. More the data more will be the prediction time.</li>\n<li><strong>Irrelevant features can fool the nearest neighbors.</strong></li>\n</ol>\n<h2><strong>KNN Implementation in Python using&nbsp;sklearn</strong></h2>\n<p>Too much theory! Let's implement the KNN algorithm in python to solve a classification problem.&nbsp;</p>\n<h3><strong>Step 1: Import the necessary dataset libraries.</strong></h3>\n<p>The dataset used to implement KNN is the famous Iris dataset imported from the Scikit-learn datasets as load_iris. Other libraries are imported for training, preprocessing, and evaluation.</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt   # update the plot \nfrom sklearn import datasets# read the data \nimport numpy as np #for arrays \nfrom sklearn.model_selection import train_test_split # split the data \nfrom sklearn.preprocessing import StandardScaler # scale the data \nfrom sklearn.neighbors import KNeighborsClassifier # the algorithm \n\nfrom sklearn.metrics import accuracy_score  #grade the results \nimport pandas as pd \n\niris = datasets.load_iris() # read the data \n\nX = iris.data[:]  # select the features to use \ny = iris.target   # select the classes\n\n\niris_dataframe = pd.DataFrame (data= np.c_[iris['data'], iris['target']],\n\n    columns= iris['feature_names'] + ['target'])\n\nplt.figure(2)\ngrr = pd.plotting.scatter_matrix(iris_dataframe,\n                                  c=iris[\"target\"], \n                                  figsize=(15, 15),\n                                  marker='o', \n                                  S=60,\n                                  alpha=.8)\nplt.show(2)\n</code></pre>\n<h3>Step 2: Understanding the&nbsp;data</h3>\n<p>This dataset has four variables: <strong>sepal length, sepal width, petal length, and petal width,</strong> describing iris plants of three types: <strong>Setosa, Versicolour,</strong> and <strong>Virginica</strong>. The dataset contains 150 observations, with each observation labeled as the actual type of the plant.&nbsp;</p>\n<h3>Step 3: Visualization</h3>\n<p>The dataset, which has four dimensions, is visualized pairwise to distinguish them. The pairwise scatter plot matrix of the iris data set helps visualize the relationship among multiple variables separately within subdivisions of the dataset. In the image below, <strong>violet</strong> color represents <strong>Setosa</strong>, <strong>green</strong> represents <strong>Versicolour</strong>, and <strong>yellow</strong> represents <strong>Virginica</strong>.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*L4b2d0tYFWeEkcDuajgZyw.jpeg\" alt=\"Pairwise comparison for different features\" title=\"Pairwise comparison for different features\"></p>\n<h3><strong>Step 4: Data Preprocessing</strong></h3>\n<p>The entire dataset is initially split into the training and testing part using the train<em>test</em>split function of Scikit-learn. A standard scaler is used in the next step, StandardScalar( ), to standardize the data (column-wise). When fit to a dataset, the function will transform the dataset to <strong>mean μ = 0</strong> and <strong>standard deviation σ = 1.</strong></p>\n<p>A dataset with having N samples and m features,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*L_Q44QKznAFWYejvWXyKoQ.png\" alt=\"Distance Calculation\" title=\"Distance Calculation\"></p>\n<p>Thus every data is then updated as,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*UkLkymXlo54_RbAseOBr1Q.png\" alt=\"Standardization\" title=\"Standardization\"></p>\n<pre><code class=\"language-python\">X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,random_state=0)\n\n\nSC = StandardScaler()# create the standard scaler \nsc.fit(X_train) # fit to the training data \nx_train_std = sc.transform(X_train) # transform the training data \nX_test_std = sc.transform(X_test) # same transformation on test data\n</code></pre>\n<h3><strong>Step 5: Model Fitting and Evaluation</strong></h3>\n<p>We will fit the KNN model for different values of K ranging between 1 to the number of samples in the testing dataset. The metric <strong>\"Minkowski\"</strong> along with <strong>p = 2</strong> represents the Euclidean distance in the R-space. The model will be fitted on different values of K and then is used to predict the output for a test sample size.</p>\n<pre><code class=\"language-python\">accuracyTest = {}; accuracy Train = {} \n\nfor k in range (len (y_test):\n\n    knn = KNeighborsClassifier(n_neighbors=k+1, p=2, metric='minkowski')\n    knn.fit(X_train_std,y_train)\n    y_pred = knn.predict(x_test_std) \n    y_train_pred = knn.predict(X_train_std) \n\n    if (k+1)%10==0:\n        print(10*'-')\n        print(\"For k = %s\" %(k+1))\n        print('Number in test ', len(y_test))\n        print('Misclassified samples: %d' % (y_test != y_pred).sum())\n\n    accTrain = accuracy_score(y_train,y_train_pred)\n    acc = accuracy_score(y_test, y_pred)\n    accuracyTest[k+1] = acc\n    accuracyTrain[k+1] = accTrain\n\nfor accuracy in [accuracy Train, accuracy Test]:\n    lists = sorted(accuracy.items() # sorted by key, return a list of tuples \n    X, y = zip(*lists) # unpack a list of pairs into two tuples \n    plt.plot(x, y)\n    plt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*2w5zsVeff7Ob1tMp4p8E2Q.png\" alt=\"Training and Testing accuracy\" title=\"Training and Testing accuracy\"></p>\n<p>If we give priority to the testing accuracy, the value of K &gt; 18 decreases the testing accuracy sharply. So we can say that the optimal number of neighbors can be around 15 to 18.</p>\n<h2><strong>Decision Boundaries for&nbsp;KNN</strong></h2>\n<p>The two datasets (training and testing) are combined to show the effect of varying K in the KNN algorithm. Only two features (petal length and petal width) are considered for visualization. The value of K taken is [1,25,50,75,100,112], where the training sample size is 112. The decision boundary at K = 112 returns the majority of the three classes, which is red.</p>\n<pre><code class=\"language-python\">X = iris.data[:, [2,3]] # select the features to use \ny = iris.target         # select the classes\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,random_state=0)\n\n\nSC = StandardScaler()# create the standard scaler \nsc.fit(X_train) # fit to the training data \nx_train_std = sc.transform(X_train) # transform the training data \nX_test_std = sc.transform(X_test) # same transformation on test data\n\nX_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))\n\nprint('Number in combined ', len(y_combined))\n# check results on combined data \ny_combined_pred = knn.predict(X_combined_std)\n\nprint('Misclassified combined samples: %d' 1 % (y_combined != y combined_pred). sum )\nprint('Combined Accuracy: %.2f' % accuracy_score(y_combined, y_combined_pred)) \n# visualize the results \n\nfor k in [1,25,50, 100, len(X_train)]:\n\n    knn = KNeighborsClassifier (n_neighbors=k, p=2, metric='minkowski')\n\n    knn.fit(X_train_std, y_train) \n\n    plot_decision_regions(X=X_combined_std, y=y_combined, classifier=knn,\n                                test_idx=range(105,150))\n\n    plt.xlabel('petal length [standardized]') \n    plt.ylabel('petal width [standardized]') \n    plt.title('k=%s'%k) \n    plt.legend(loc='upper left') \n    plt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*VXDGlFehAYqClSiAbsApSQ.jpeg\" alt=\"Decision boundary\" title=\"Decision boundary\"></p>\n<h2>Industrial Applications of&nbsp;KNN</h2>\n<p>Although there are certain limitations, this algorithm is widely used in industries because of its simplicity. Some of these applications are:</p>\n<ol>\n<li><strong>Email spam filtering:</strong> For detecting the trivial and fixed types of spam emails, KNN can perform well. The implementation steps of this algorithm can be found <a href=\"https://www.enjoyalgorithms.com/blog/email-spam-and-non-spam-filtering-using-machine-learning\">here</a>.</li>\n<li><strong>Wine Quality prediction:</strong> Wine quality prediction is a regression task and can be solved using the KNN algorithm. The implementation can be found <a href=\"https://www.enjoyalgorithms.com/blog/wine-quality-prediction\">here</a>.</li>\n<li><strong>Recommendation system:</strong> KNN is used to build the recommendation engines that recommend some products/movies/songs to the users based on their likings or disliking.&nbsp;</li>\n</ol>\n<h2>Possible Interview Questions</h2>\n<p>As we stated, this algorithm brings a lot of explainability with itself. Interviewers can ask more profound questions on this topic. Some of them could be,</p>\n<ol>\n<li>How k-NN is different from other Machine Learning algorithms?</li>\n<li>Will changing the distance metric affect the classification accuracy?</li>\n<li>Is k-NN highly sensitive to data normalization?&nbsp;</li>\n<li>Why is it a non-parametric algorithm?</li>\n<li>What are the major cons of the k-NN algorithm?</li>\n</ol>\n<h2>Conclusion</h2>\n<p>In this article, we have covered the concept of the first \"Machine Learning\" algorithm, i.e., KNearest Neighbour. We saw how we can define the instances as neighbors and how the value of K affects the predictions. We also discussed why feature scaling played a vital role and learned about the Voronoi Diagram. After that, we discussed the regression use-case of KNN. Finally, we implemented the KNN algorithm on the famous Iris dataset. We hope you enjoyed the article.</p>\n<h4>References</h4>\n<ol>\n<li><a href=\"http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\">Scikit-learn: Machine Learning in Python</a>, Pedregosa, <em>et al.</em>, JMLR 12, pp. 2825–2830, 2011</li>\n<li>Mitchell, T. M. (1997). Machine learning., McGraw Hill series in computer science New York: McGraw-Hill.</li>\n<li>UCI Machine Learning Repository: Iris Data Set.</li>\n<li>J. D. Hunter, “Matplotlib: A 2D Graphics Environment”, Computing in Science &amp; Engineering, vol. 9, no. 3, pp. 90–95, 2007.</li>\n</ol>\n<h4>Enjoy Learning! Enjoy Algorithms!</h4></div>",
        "Description": "K-Nearest Neighbor is a Supervised Machine Learning algorithm that can be used to solve both classification and regression problems. The explainability it brings is rare to find among other advanced algorithms.",
        "AuthorAndReviewer": "Ravish Raj",
        "Tag": "MachineLearning",
        "DomainFlagTag": "MachineLearning",
        "coverImage": "https://www.enjoyalgorithms.com/static/k-nearest-neighbours-in-ml-cover-a663debdb2d2f9c372ebb2659ee0fa04.png"
      }
    },
    {
      "id": 11,
      "attributes": {
        "Heading": "K-Means Clustering Algorithm In Machine Learning",
        "createdAt": "2022-03-07T05:06:01.512Z",
        "updatedAt": "2022-03-07T05:06:02.820Z",
        "publishedAt": "2022-03-07T05:06:02.818Z",
        "PublishedDate": "2022-02-19",
        "ContentBody": "<div class=\"mt-6 ml-0 max-w-none md:ml-10 prose xl:prose-lg select-none\"><p>Data understanding is very crucial in the field of Machine Learning. With the recent trend of collecting data, every organization has started collecting some form of it. But the concern is, most of these data are unutilized and considered digital junk. Labeling this vast amount of data with the help of annotation engineers or data engineers would be time-consuming and highly expensive. We can say the quality of data is directly proportional to the cost it acquires. But we know that the budget is always limited.</p>\n<p>If we can not spend too much on manpower required to label the data, why not build algorithms that can provide us information without manual intervention? In such a scenario, unsupervised learning algorithms come into existence. They help us extract meaningful information from the junk and present it in a smooth human-readable format without any supervision. The absolute difference between Supervised and Unsupervised algorithms is explained in <a href=\"https://www.enjoyalgorithms.com/blogs/supervised-unsupervised-and-semisupervised-learning\">this blog</a>.</p>\n<h2><strong>Key takeaways from this blog:</strong></h2>\n<p>After going through this blog, we will be able to understand the following things:</p>\n<ol>\n<li>What is clustering in machine learning?</li>\n<li>What is meant by the K-means algorithm?&nbsp;</li>\n<li>What are the basic steps of K-means clustering?</li>\n<li>What are the limitations of K-means?</li>\n<li>What are the possible improvements in K-means?</li>\n<li>How to choose the value of K in K-means?</li>\n<li>What are some inherent problems with clustering algorithms?</li>\n</ol>\n<p>K-means algorithm is a type of clustering algorithm; hence, before moving towards the depth of this algorithm, let's first understand the term \"Clustering.\"</p>\n<h2>What is clustering in machine learning?</h2>\n<p>If we define this term formally,</p>\n<blockquote>\n<p><strong>Clustering is an unsupervised learning technique which is used to find the subgroups (also known as Clusters) in the&nbsp;dataset.</strong></p>\n</blockquote>\n<p>Let's understand it in a layman form. Suppose we have three flavored candies in a box, Salty, Sweet, and Chocolate. Earlier, all these candies were mixed in the box, and every candy had the same shape and size. Now suppose a kid asks us to give a chocolate candy. As the candies are mixed, this task will take time. We can separate these candies into three different boxes to make this easier, each corresponding to one taste. This way of separating candies is known as clustering.&nbsp;</p>\n<p>In the image below, data samples have been divided into three clusters, and each cluster has been assigned different colors. We can say that yellow squares correspond to the 1st cluster, blue corresponds to the 2nd, and red corresponds to the 3rd.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*IacpKQPOw7JYKv0d3rkSQg.png\" alt=\"clustering in three clusters\" title=\"clustering in three clusters\"></p>\n<p>The clustering technique is prevalent in many fields, and hence there exist many algorithms to perform it. K-means is one of them.&nbsp;</p>\n<h3>What is meant by the K-means algorithm?</h3>\n<p>James MacQueen first used the term \"K-means\" in 1967. It is an older technique but still very popular in the data science and machine learning industries. If we define the term formally,</p>\n<blockquote>\n<p>K-means is a simple and elegant approach which is used to partition data samples into a pre-defined “<strong>K</strong>“ <strong>distinct and non-overlapping</strong> clusters.</p>\n</blockquote>\n<p>The value of K in the K-means algorithm depends upon the user's choice. In the image above, the user has defined the value of K = 3. Every observation from the dataset will follow two fundamental properties,</p>\n<ol>\n<li><strong>Each observation belongs to at least one of the K clusters.</strong> In simple terms, there can't be any sample that is not a part of any of the clusters.</li>\n<li><strong>No observation will belong to more than one cluster.</strong> In simple terms, one sample can not lie in two different clusters simultaneously.</li>\n</ol>\n<p>Because of its <strong>simplicity</strong>, <strong>relative robustness</strong> (work with a wider variety of datasets), and \"<strong>good enough</strong>\" explanation, it is still considered one of the first algorithms that data analysts use to investigate any new dataset.</p>\n<h2>What are the basic steps of K-means clustering?</h2>\n<p>Let's suppose we have some data samples (X1, X2,&nbsp;…, Xn), and we want to divide these data samples into \"K\" different clusters. We can use the K-means algorithm to perform this work based on iterative logic. The complete implementation can be summarized in five steps.</p>\n<p><strong>Step 1:</strong> Accept inputs of data and the number of clusters to group that data. There are <strong>n</strong> data samples in our case, and we want to divide these samples into <strong>k</strong> different clusters.</p>\n<p><strong>Step 2:</strong> Initialize the first K clusters. There can be two ways to do that,</p>\n<ul>\n<li>Pick first K samples, or&nbsp;</li>\n<li>Take Random sampling of K elements from the available dataset.</li>\n</ul>\n<p>These <strong>K</strong> samples will be treated as temporary centroids (also called mean).</p>\n<p><strong>Step 3:</strong> Now we have <strong>(n-k)</strong> data samples left. Based on the proximity/distance to the K centroid values, each data sample from (n-k) will be assigned to only one of the K clusters. At this stage, all the samples have been assigned to some cluster.</p>\n<ul>\n<li>Each record is assigned to the nearest cluster using a measure of distance (e.g., Euclidean distance)</li>\n</ul>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/0*Mbgg1TZAvSMv9Vq_.png\" alt=\"Distance between two points \" title=\"Distance between two points \"></p>\n<p><strong>Step 4:</strong> We need to calculate the new centroids for every cluster. But this time, the centroid calculation will not be random, but the samples present in the cluster will be averaged to calculate the mean. At this step, we will have <strong>K</strong> centroids which can be different from the earlier chosen random <strong>K</strong> samples<strong>.</strong></p>\n<p><strong>Step 5:</strong> We will iterate over this algorithm starting from Step 3 for the entire <strong>n</strong> samples. This will be repeated until we see no/negligible movement of samples among clusters.&nbsp;</p>\n<p>Via this way, K-means groups the data samples into the desired number of clusters. The steps are very straightforward and very intuitive. This can be considered one of the most important reasons for the popularity of this algorithm.</p>\n<h2>What are the limitations of&nbsp;K-means?</h2>\n<p>K-means is a vital algorithm, but it has certain limitations as well. Some of these limitations are:</p>\n<ul>\n<li>Looking carefully at the steps mentioned above, we can say that this algorithm is computationally expensive. It requires time proportional to the product of the <strong>number of data items (n)</strong>, <strong>number of clusters (k)</strong>, and the <strong>number of iterations (m).</strong></li>\n<li>In the first step, we picked K clusters randomly, because of which K-means generally converges to a local optimum. The quality of the resulting clusters highly depends upon the selection of initial <strong>K</strong> clusters.</li>\n<li>There can be situations where this algorithm can suffer from an empty cluster problem. If we remember, we picked K initial values for K centroids randomly, and we may probably pick outlier here. In such conditions, the cluster might become empty.</li>\n</ul>\n<h2>What are the possible improvements in&nbsp;K-means?</h2>\n<p>As per the limitations discussed above, there are scopes where K-means can be improved further. The K-means algorithm computes the distance between data samples and the centroid within each cluster in each iteration. It is making this algorithm computationally very expansive in the case of huge datasets. However, we can utilize the knowledge of the distance calculated in the previous iteration to make it computationally cheaper. <strong>Can you think how?</strong></p>\n<p>In the first iteration, we calculated the distance for each data point to the nearest cluster, termed it as <strong>previous_distance[i]</strong> for <strong>ith</strong> sample. At the next iteration, centroids of all the clusters will change because samples will change their clusters. Here, we will first calculate the distance of the same <strong>ith</strong> sample to the previous cluster (<strong>but with the updated centroid</strong>) and termed it as <strong>current_distance[i]</strong>. Now we will compare it with the previous distance <strong>previous_distance[i]</strong>. There will be two scenarios:</p>\n<ol>\n<li><strong>current<em>distance[i] ≤ previous</em>distance[i]&nbsp;:</strong> There will be no movement of the ith sample within clusters. We can interpret it with the situation that the updated centroid of the cluster came closer to the ith sample, and hence it became a more potential candidate for falling into that cluster.</li>\n<li><strong>current<em>distance[i] &gt;previous</em>distance[i]&nbsp;:</strong> Sample will leave the previous cluster and move towards the other cluster.</li>\n</ol>\n<p>With this comparison in the start, we will be saving the time required to compute the distances to <strong>K-1</strong> clusters, and it becomes slightly better in terms of computational complexity.</p>\n<h2>How to decide the number of clusters \"K\" in the K-means algorithm?</h2>\n<p>The decision of an optimal number of clusters is subjective and depends upon the methods to find the similarity based on which we performed the clustering. There are some algorithmic methods also that can help us in making decisions on K,</p>\n<h3><strong>Elbow method in&nbsp;K-means</strong></h3>\n<p>Let's discuss the pseudo-code using which we can implement the Elbow method.</p>\n<ol>\n<li>Run the K-means algorithm for different values of K, let's say K=1 to K=10.</li>\n<li>For every K, calculate the total sum of distances of samples to their closest cluster centroid. Let's say it SSE (Sum Squared Error). We say it is an error because ideally, every sample should lie on the centroid of the corresponding cluster, which is not possible.</li>\n<li>Plot the curve of SSE with respect to the number of clusters K.</li>\n<li>The value of <strong>K</strong> at the bend (knee) in the plot is generally considered an indicator of the appropriate number of clusters.</li>\n</ol>\n<pre><code class=\"language-python\">SSE = {}\nfor k in range(1, 10):\n    k_means = KMeans(n_clusters=k, max_iter=1000).fit(data)\n    data[\"clusters\"] = kmeans.labels_\n    SSE[k] = kmeans.inertia_\n  \n# Inertia: Sum of distances of samples to their closest cluster #center\nplt.figure()\nplt.plot(list(SSE.keys()), list(SSE.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*Z1-c3GKKA5rZKxbCu-wYkw.png\" alt=\"Elbow point\" title=\"Elbow point\"></p>\n<p>We can say that the optimal value of clusters is 3 for the data used in the above code. There is another famous method,</p>\n<h3>Average silhouette method in&nbsp;K-means</h3>\n<p>Let's learn the pseudo-code of this algorithm as well.</p>\n<ol>\n<li>Run the K-means algorithm for different values of K, let's say K=2 to K=11.</li>\n<li>For every K, calculate the <a href=\"https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\">average silhouette of observations</a>. Let's say AvS.</li>\n<li>Plot the curve of AvS with respect to the number of clusters K.</li>\n<li>The value of <strong>K</strong> at the maximum AvS will be considered optimal.</li>\n</ol>\n<pre><code class=\"language-python\">for n_cluster in range(2, 11):\n    kmeans = KMeans(n_clusters=n_cluster).fit(data)\n    label = kmeans.labels_\n    AvS = silhouette_score(data, label, metric='euclidean')\n    print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, AvS))\n\n    \n##############################################################\n'''\nFor n_clusters=2, The Silhouette Coefficient is 0.680813620271\nFor n_clusters=3, The Silhouette Coefficient is 0.552591944521\nFor n_clusters=4, The Silhouette Coefficient is 0.496992849949\nFor n_clusters=5, The Silhouette Coefficient is 0.488517550854\nFor n_clusters=6, The Silhouette Coefficient is 0.370380309351\nFor n_clusters=7, The Silhouette Coefficient is 0.356303270516\nFor n_clusters=8, The Silhouette Coefficient is 0.365164535737\nFor n_clusters=9, The Silhouette Coefficient is 0.346583642095\nFor n_clusters=10, The Silhouette Coefficient is 0.328266088778\n\n'''\n###############################################################\n</code></pre>\n<p>Here, n = 2 has the highest coefficient, so ideally, it should be selected, but, based on the knowledge of the Iris data, we were aware that three species are there, so the next value of 3 is chosen for optimal. When dealing with higher dimensions, this method produces a better result. From this example, we can say that these methods can only help us decide the appropriate number of clusters, and we can not wholly rely on their predictions.</p>\n<h2>Implementation of K-means in&nbsp;python</h2>\n<p>Let's try to implement this algorithm using the Scikit-Learn library on one of the famous datasets of the framework, i.e., the Iris Dataset shown in the image below.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1440/1*PgzMug4wqKV9Z5jBgV0LUg.jpeg\" alt=\"Iris dataset\" title=\"Iris dataset\"></p>\n<p><strong>Step 1:</strong> Importing the required libraries of pandas, Iris datasets, KMeans model, and matplotlib.</p>\n<pre><code class=\"language-python\">import pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n</code></pre>\n<p><strong>Step 2:</strong> Extract the data from the load_iris dataset and separate the target variables.</p>\n<pre><code class=\"language-python\">iris = load_iris()\nX = iris.data\ny = iris.target\n</code></pre>\n<p><strong>Step 3:</strong> Fit the Scikit-learn K-means model</p>\n<pre><code class=\"language-python\">kmeans = KMeans(n_clusters=3, max_iter=1000).fit(X)\nlabels = kmeans.labels_\n</code></pre>\n<p><strong>Step 4:</strong> Plot the scattered pred</p>\n<pre><code class=\"language-python\">plt.figure(figsize=(4, 3))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nax.scatter(X[:, 3], X[:, 0], X[:, 2],\n           c=labels.astype(float), edgecolor='k')\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Petal width')\nax.set_ylabel('Sepal length')\nax.set_zlabel('Petal length')\nax.dist = 12\nplt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1440/1*vEQ-539FpVm-i6rT6Ze7XQ.jpeg\" alt=\"3D Plot of iris data cluster\" title=\"3D Plot of iris data cluster\"></p>\n<p>We can see that the 3 clusters are formed based on the Petal width, Petal Length, and Sepal Length. From the Average silhouette method, we received the best number of clusters is 2, and we also noticed from the above image that yellow and purple samples are very close. That's why the silhouette method got confused and suggested two clusters.</p>\n<h2>Practical Issues in Clustering</h2>\n<p>There is no doubt that clustering is one of the most important processes for data science and machine learning, but it has some practical challenges for which the solution is challenging to find. Some of these practical issues are</p>\n<ol>\n<li>Performing normalization or standardization before applying cluster sometimes produce better results and sometimes worse. So this decision is not firm and only be observed by actually implementing it. And the final clusters are highly dependent upon these steps.</li>\n<li>Choice of the number of clusters is not fixed and only can be observed by actually implementing it.</li>\n<li>The results of the clustering algorithms are hard to analyze in the case of higher dimensions. Dimensionality reduction methods like <a href=\"https://www.enjoyalgorithms.com/blog/principal-component-analysis-in-ml\">PCA</a> can be used to reduce the dimensions here, but it also causes data loss.</li>\n</ol>\n<h2>Industrial Applications of&nbsp;K-means</h2>\n<p>K-means is a very popular algorithm widely used to analyze real-life data, including medical, sales, finance, and many more. Some of the most excellent applications of K-means are:</p>\n<ol>\n<li>This algorithm is used to construct the Yolo-v3,4,5 object detection models, which are very famous in deep learning.</li>\n<li>Customer Segmentation is one of the most incredible examples which uses K-means directly. More details can be found <a href=\"https://www.enjoyalgorithms.com/blog/customer-segmentation-using-hierarchical-clustering\">here</a>.</li>\n<li>Pattern recognition in images: We can feed millions and billions of pictures into this algorithm. K-means can segregate them into the desired number of clusters based on their content.</li>\n</ol>\n<h2>Possible Interview Questions</h2>\n<p>K-means is considered one of the fundamental algorithms that every interviewer expects that candidate knows about it. Data analysis and finding patterns in the massive chunk of data is the most crucial step, and knowledge of k-means suggests that the data scientist position candidates have worked on unstructured data. Possible interview questions on this topic would be,</p>\n<ol>\n<li>What is the step-wise process in which K-means find clusters?</li>\n<li>How do we decide the number of clusters?</li>\n<li>How can we improve the speed of the native K-means algorithm?</li>\n<li>What are the inherent problems of clustering algorithms?</li>\n<li>What are the other methods used for clustering?</li>\n</ol>\n<h2>Conclusion</h2>\n<p>In this article, we discussed the K-means algorithm in a detailed fashion. We learned the pseudo-code of K-means algorithms, inherent problems associated with the K-means algorithm, and how we can improve the time performance of the K-means algorithm. We discussed methods that can help find the optimal values K, one of the most frequent questions asked in the machine learning interviews. After that, we implemented the algorithm on the Iris dataset.</p>\n<h3>Enjoy Learning! Enjoy Clustering! Enjoy Algorithms!</h3></div>",
        "Description": "K-means is an unsupervised learning technique used to partition the data into pre-defined K partitions. These partitions are called clusters.",
        "AuthorAndReviewer": "Ravish Raj",
        "Tag": "MachineLearning",
        "DomainFlagTag": "MachineLearning",
        "coverImage": "https://www.enjoyalgorithms.com/static/k-means-clustering-algorithm-cover-5a07d159a5dc8bd3a898d3afe5fbd1ed.jpg"
      }
    }
  ],
  "meta": {
    "pagination": {
      "page": 1,
      "pageSize": 25,
      "pageCount": 1,
      "total": 3
    }
  }
}