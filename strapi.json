{
  "data": [
    {
      "id": 9,
      "attributes": {
        "Heading": "Naive Bayes Classifier",
        "createdAt": "2022-03-06T12:57:02.943Z",
        "updatedAt": "2022-03-06T12:57:04.204Z",
        "publishedAt": "2022-03-06T12:57:04.202Z",
        "PublishedDate": "2022-02-24",
        "ContentBody": "<div class=\"mt-6 ml-0 max-w-none md:ml-10 prose xl:prose-lg select-none\"><p>In Machine Learning and Data Science field, researchers have developed many advanced algorithms like Support Vector Machines, Logistic Regression, Gradient Boosting, etc. These algorithms are capable enough to produce very high accuracy. But among these advanced ones, there exists an elementary and Naive algorithm, known as Naive Bayes.&nbsp;</p>\n<p>In English, the \"Naive\" word is used for a person or action lacking experience, wisdom, or judgment. This tag is associated with the Naive Bayes algorithm because it also makes some silly assumptions while making any predictions. But the most exciting thing is it still performs better or equivalent to the best algorithms. So let's learn about this algorithm in greater detail.</p>\n<h2>Key takeaways from this&nbsp;blog</h2>\n<p>After going through this blog, we will have an understanding of the following things:</p>\n<ol>\n<li>What is Bayes theorem?</li>\n<li>Why Bayes theorem in Machine Learning?</li>\n<li>Naive Bayes examples for single and multiple features.</li>\n<li>How does Naive Bayes handle the non-categorical features?</li>\n<li>What is Gaussian Naive Bayes?</li>\n<li>Python-based implementation</li>\n<li>Advantages and disadvantages of Naive Bayes.</li>\n<li>Industrial applications of Naive Bayes.</li>\n<li>Possible Interview Questions on this topic.</li>\n</ol>\n<p>Let's start without any further delay.</p>\n<h2>The Basic Intuition of Baye's&nbsp;Theorem</h2>\n<p>In our <a href=\"https://www.enjoymathematics.com/blog/probability-theory-for-machine-learning\">probability blog</a>, we discussed Baye's theorem. Let's use the same example here.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*Vql4mVP0Q7R61Au4Nl-_2Q.jpeg\" alt=\"Probability Bayes theorem basics, source: ifunny.co\" title=\"Probability Bayes theorem basics \"></p>\n<h4>Terms in the above example can be read like&nbsp;this:</h4>\n<p>P(chill | Netflix)&nbsp;:= Probability that we enjoy if we are watching Netflix.</p>\n<p>P(Netflix | chill)&nbsp;:=Probability that we are watching Netflix if we know that we are chilling/enjoying.</p>\n<p>P(chill)&nbsp;:= Probability that we are chilling/enjoying.</p>\n<p>P(Netflix)&nbsp;:= Probability that we are watching Netflix.</p>\n<p>So according to Bayes Theorem, if we want to know the probability that we are enjoying if we are watching Netflix (i.e., P(chill | Netflix)), we must know two things:</p>\n<ul>\n<li><strong>Likelihood probabilities:</strong> The probability that we are watching Netflix if we know that we are chilling/enjoying, i.e., P(Netflix | chill)</li>\n<li><strong>Prior probabilities:</strong> Probabilities that we are enjoying and the Probability that we are watching Netflix. P(chill) and P(Netflix).</li>\n</ul>\n<h2>Why Bayes Theorem in Machine Learning?</h2>\n<p>Let's try to find the answer to this question. In the case of supervised learning, we have input features and the corresponding output labels present with us. We try to make our machines learn the relationship between input features to the output variable. Once this learning is done on training data, we can use this model to make predictions on test data. Let's represent this supervised approach in a Bayesian format.&nbsp;</p>\n<p>We know the feature values for test data and want to see the output label if that particular values of features are present. We can also say we want to predict the chances of occurrence of any label if the values of features are already known. That is precisely the same as <strong>p(label | feature).</strong></p>\n<p>From the Bayes theorem, to know the value of p(label | feature), we must know likelihood probabilities p(feature | label) and the prior probabilities, p(label) and p(features). <strong>But, do we really have these values?</strong></p>\n<p>Yes! from the training data. That's the whole crux of supervised learning. Right?</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*EIVtLAe05L4htlZeMc8OFA.jpeg\" alt=\"baye's theorem terms \" title=\"baye's theorem terms \"></p>\n<p>Let's discuss the terms on the right side in the formulae above. The likelihood term p(feature | label) says the probability of that feature if we already know the label. And for training data, we know the label for each sample. Also, the prior probabilities p(label) and p(feature) can be calculated from the training data. So ultimately, we will have the posterior probability that we wanted to calculate.</p>\n<h2>Naive Bayes&nbsp;example</h2>\n<h3>Single Feature</h3>\n<p>Let's take the example of a football game, and below is the data which says if humidity is high or normal, then play happens; otherwise, if humidity is low, the play does not occur. Straightforward data, correct?</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*dEgAxCZ456u4TTUYlDrwAQ.png\" alt=\"Data snippet\" title=\"Data snippet\"></p>\n<p>Suppose we want to make a machine learning model, which receives the feature value of humidity and tries to predict whether the play will happen or not. So suppose we know the humidity as Normal, and let's calculate the chances of play to happen, i.e., p(play = Yes | humidity = Normal),</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*PK1H1tjDObbWJHw-Z_QTow.png\" alt=\"Estimating posetrior probability \" title=\"Estimating posetrior probability \"></p>\n<p>From the data,&nbsp;</p>\n<p><strong>p(humidity = Normal | play = Yes)</strong>&nbsp;:= Probability of humidity to be normal when we know that play happened is 1/2, as there are two cases when the play happened. One is when humidity = High and Humidity = Normal. So the probability of humidity = Normal will be 0.5.</p>\n<p><strong>p(play = Yes)&nbsp;:=</strong> Probability of game to have happened will be 2/3 as we have 3 samples out of which 2 says the play happened as \"Yes\".</p>\n<p><strong>p(humidity = Normal)&nbsp;:=</strong> probability of humidity to be normal will be 1/3 as there are three samples out of which 1 instance is of humidity = Normal.</p>\n<p>So,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*3AKJx9uS4hZ2dwQJag27Xg.png\" alt=\"Estimating posetrior probability 2\" title=\"Estimating posetrior probability 2\"></p>\n<p>The same can be understood from the data intuition as well. Similarly, p(play= No | humidity = Normal) can be calculated, and it will be 0.</p>\n<h3>Multiple Feature</h3>\n<p>The example we saw above had just one feature, the humidity value. But in the practical, real-life data, we will have multiple features. In such a case, the same equation can be represented as,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*7EDIZA-m1M5_pVgEbJ4C7Q.png\" alt=\" posetrior probability for multiple features\" title=\" posetrior probability for multiple features\"></p>\n<p>Expanding the above equation,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*qXHGflbJly7-NQ_0cuc4HA.png\" alt=\" posetrior probability for multiple features 2\" title=\" posetrior probability for multiple features 2\"></p>\n<p>This calculation is extensive and expansive in terms of computations. That's where the Naïve Bayes algorithms come with their \"naive\" assumption. It considers every feature is independent of each other, which means one feature is unaffected by the occurrence of any other feature.&nbsp;</p>\n<p>This assumption is impractical in real-life scenarios. Suppose we are recording features from our cell phone's battery, the terminal voltage, current, and temperature. Using these, we estimate whether battery health is \"good\" or \"bad\". It's a classification problem statement, and algorithms like SVM, logistic regression, etc., will learn mapping functions between feature sets and labels. While learning, these algorithms will not assume that current is independent of the Voltage feature, which is oblivious. Voltage produces the current, which results in the heating of the battery, so temperature varies accordingly. So all features are dependent.&nbsp;</p>\n<p>But Naive Bayes algorithm assumes that all three features are independent, and the occurrence of one feature is totally unaffected by the occurrence of others. This is not true in practical cases. Still, this algorithm produces quite fascinating results. According to Naïve Bayes, the above equation is modified as follows:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*fBQzL_cQ_RvJTV9aJcwiEA.png\" alt=\" posetrior probability for multiple features 3\" title=\" posetrior probability for multiple features 3\"></p>\n<p>This assumption drastically reduces the computation cost and also delivers good accuracy. Also, in the Naive Bayes algorithm, we don't calculate the denominator of the above equation to save some computation power. Because, for all the classes, the denominator term remains the same and does not make any contribution in segregating different classes. Hence, we can represent the above equation as:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*OA21o89W2WGfh61CItKMdA.png\" alt=\" posetrior probability for multiple features 4\" title=\" posetrior probability for multiple features 4\"></p>\n<h2>How does Naive Bayes handle the non-categorical features?</h2>\n<p>What if the features are continuous numerical values (non-categorical). How will we estimate probabilities in such a case?&nbsp;</p>\n<p>Here comes the theory of <a href=\"https://www.enjoymathematics.com/blog/probability-distribution-function-for-machine-learning\">Probability Distribution Function</a> (PDF). We need to estimate the probabilities for numerical variables using this concept.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*k_LvuyKjbN1Gr54aZ06GGw.png\" alt=\"Gaussian distribution continuous vs discrete\" title=\"Gaussian distribution continuous vs discrete\"></p>\n<p>Suppose we assume that the PDF is a gaussian or normal distribution. In that case, we need to calculate the <strong>mean(μ)</strong> and <strong>standard deviation (σ)</strong> of that feature,and then for any value of feature x, the probability <strong>f(x)</strong> can be calculated from the below equation.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*eC6_xSUmSbshBxsCtpl-pQ.png\" alt=\"Normal Distribution formulae\" title=\"Normal Distribution formulae\"></p>\n<h2><strong>Gaussian Naïve&nbsp;Bayes</strong></h2>\n<p>In the case of gaussian distribution assumption, we call Naïve Bayes, <strong>Gaussian Naïve Bayes</strong> algorithm. We can choose other probability distribution functions, like Bernoulli, Binomial, etc., to estimate probabilities. Among all these, gaussian is the most famous one as most of the real-world sensors produce data in the gaussian distribution format. If we look at the image below closely, most people in the gym use 15–20 Kg weights. Later on, the usage of other weights decreases subsequently, like what we observe in the gaussian distribution.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*8OeMInFFHZ4_NrO9uAOq_w.jpeg\" alt=\"Real-life normal distribution\" title=\"Real-life normal distribution\"></p>\n<p>Too much theory, let's implement it on a practical machine learning application, i.e., Sentiment analysis.</p>\n<h2>Can we solve regression problems using Naive&nbsp;Bayes?</h2>\n<p>Although the native form of the Naive Bayes algorithm is made to solve the classification problems only, a team tried to solve the regression problem using Naive Bayes, and the work can be found <a href=\"https://link.springer.com/content/pdf/10.1023%2FA%3A1007670802811.pdf\">here</a>. They concluded that they did not achieve good results and suggested that Naive Bayes is suitable for classification algorithms.</p>\n<h2>Naive Bayes Sklearn In&nbsp;Python</h2>\n<p>Let's train a Naive Bayes algorithm on the famous Iris dataset. The objective of our algorithm would be to look onto the available features like Sepal/Petal length/width and classify flowers into three categories of Setosa, Versicolor, and Virginica.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*na4VvO08G-ec0RDulgBOLQ.jpeg\" alt=\"Iris data visualization\" title=\"Iris data visualization\"></p>\n<h3>Step 1: Importing required libraries</h3>\n<p>Standard libraries like numpy (for numeric operations), pandas (for managing the data), and matplotlib (to visualize the analysis of dataset) would be required.</p>\n<pre><code class=\"language-python\">import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</code></pre>\n<h3>Step 2: Loading the dataset and visualize scatter-plot of&nbsp;features</h3>\n<p>The famous iris dataset comes with the Scikit-learn library. If we print the features present in the dataset, the output is ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']. Also, the target variable y can take three values [0, 1, 2] corresponding to three flower classes.</p>\n<pre><code class=\"language-python\">from sklearn import datasets\n\niris = datasets.load_iris()            #loading dataset\n\nX = iris.data[:,]                    #input\ny = iris.target                      #target\n\nprint(\"Features : \", iris['feature_names'])\n\niris_dataframe = pd.DataFrame(data=np.c_[iris['data'],iris['target']],\n                             columns=iris['feature_names']+['target'])\n\nplt.figure()\ngrr = pd.plotting.scatter_matrix(iris_dataframe, c=iris['target'],\n                                  figsize=(15,5),\n                                  s=60,alpha=8)\nplt.show()\n\n# Features :  ['sepal length (cm)', 'sepal width (cm)', 'petal      # length (cm)', 'petal width (cm)']\n</code></pre>\n<p>The code above will plot the diagram shown below.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*FMtANlM6hRH7-NCJqfaiHg.jpeg\" alt=\"IRIS Data pair plot\" title=\"IRIS Data pair plot\"></p>\n<h3>Step 3: Visualize the correlation and check the assumption of Naive&nbsp;Bayes</h3>\n<p>We can plot the correlation matrix using the seaborn library.</p>\n<pre><code class=\"language-python\">import seaborn as sns\ndataplot = sns.heatmap(iris_dataframe.corr(), annot=True)\n\nplt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*GLRwmL4iQoRITJRZCNIzog.jpeg\" alt=\"Cross correlation between features\" title=\"Cross correlation between features\"></p>\n<p>We can see that the features are highly correlated. But as per Naive Bayes assumption, it will treat features as entirely independent to each other. Based on this, our algorithm will compute the following probability for all three flower classes.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*f6w3BLQiF_AV_85l7lHXUg.png\" alt=\"Naive bayes for iris data\" title=\"Naive bayes for iris data\"></p>\n<h3>Step 4: Split the&nbsp;dataset</h3>\n<p>Now we are in a position where we can segregate our training and testing data. For that, we can use the inbuilt function of train<em>test</em>split from Sklearn.</p>\n<pre><code class=\"language-python\">from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n</code></pre>\n<h3>Step 5: Fit the&nbsp;model</h3>\n<p>Now, assume our data follows Gaussian distribution and import the Gaussian Naive Bayes <strong>GaussianNB</strong> model from Sklearn. Let's fit this model into our training data.</p>\n<pre><code class=\"language-python\">from sklearn.naive_bayes import GaussianNB\n\nNB = GaussianNB()\nNB.fit(X_train, y_train)\n</code></pre>\n<p>Hurrah! We have our model ready with us now.</p>\n<h3><strong>Step 6: Evaluate the&nbsp;model</strong></h3>\n<p>We must know that we have solved a classification problem to evaluate the model. Hence some standard metrics for evaluating our model can be accuracy, precision, recall, F-1 Score, etc. A detailed list can be found here in this <a href=\"https://www.enjoyalgorithms.com/blog/evaluation-metrics-classification-models\">blog</a>. As we know, all these matrices can be calculated by having the confusion matrix ready with us. So let's plot the confusion matrix.</p>\n<pre><code class=\"language-python\">Y_pred = NB.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, Y_pred)\n\ndf_cm = pd.DataFrame(cm, columns=np.unique(y_test), index = np.unique(y_test))\n\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\n\nsns.heatmap(df_cm, annot=True)# font size\nplt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*TzjVqa6_8gfiLlushOJw_g.jpeg\" alt=\"Confusion matrix for naive bayes\" title=\"Confusion matrix for naive bayes\"></p>\n<p>From the matrix, we can see that the model quickly achieved 100% accuracy on test data. This is really amazing. It started with the silly assumption but provided whatever we wanted. That's the main reason why this algorithm is famous among researchers.</p>\n<h2>Advantages of Naive&nbsp;Bayes</h2>\n<ol>\n<li><strong>Computationally Simple:</strong> This classifier is computationally very simple compared to algorithms like SVM and XGBoost.&nbsp;</li>\n<li>When the independent nature of features becomes true in data, the Naive Bayes algorithm performs the best and can beat the accuracy of <a href=\"https://www.enjoyalgorithms.com/blog/logistic-regression-in-ml\">Logistic Regression</a>.</li>\n<li><strong>Multi-class prediction:</strong> It can provide us the probabilities do different classes and hence can give multi-class predictions.</li>\n<li>It works best in the case of Categorical variables compared to the numerical ones.</li>\n</ol>\n<h2>Disadvantages of Naive&nbsp;Bayes</h2>\n<ol>\n<li><strong>Independent treatment of features:</strong> One of the most significant disadvantages of this algorithm is that it treats every feature independently, which can be cumbersome most of the time. Advanced classifiers like SVMs learn the relationship between the feature and target variables and the relationship among features. This is not the case with Naive Bayes.</li>\n<li><strong>Zero frequency case:</strong> Probably, some values from a categorical variable do not appear in training data. In such a case, Naive Bayes will assign a zero probability to these samples when they appear in test data. This is called \"Zero Frequency\" and can be cured using smoothing techniques, like <a href=\"https://en.wikipedia.org/wiki/Additive_smoothing\">Laplace estimation</a>.</li>\n<li><strong>Bad Estimator:</strong> Naive Bayes is considered to be a lousy probability estimator, so we should not take the probability predicted by this algorithm too seriously.&nbsp;</li>\n</ol>\n<h2>Industrial Applications of Naive&nbsp;Bayes</h2>\n<p>This algorithm is naive, but because of its performance, several industrial applications are based on this.</p>\n<ol>\n<li><strong>Recommendation System:</strong> <a href=\"https://www.enjoyalgorithms.com/blog/recommender-system-using-machine-learning\">Collaborative Filtering</a> and the Naive Bayes algorithm form the recommendation system that can recommend some products/movies/songs to users.</li>\n<li><strong>Multi-class prediction:</strong> One of the most significant advantages of Naive Bayes is its applicability in predicting the probability of multiple classes.&nbsp;</li>\n<li><strong>Real-time prediction:</strong> Because of its lesser complex nature, it can give prediction results very fast. Many advanced algorithms like SVM XGBoost are too heavy sometimes for smaller classification tasks. In such a scenario, Naive Bayes performs well.</li>\n</ol>\n<h2>Possible Interview Question</h2>\n<p>The theory of this algorithm is really important as it is rare to find full-scale projects in resumes related to this algorithm. Interviewers want to test the probability knowledge from this topic. Some popular questions can be:</p>\n<ol>\n<li>Explain the Bayes Theorem and associate it with the Naive Bayes algorithm.</li>\n<li>Why is this algorithm called Naive?</li>\n<li>How do these probability calculations hold for continuous numerical variables?</li>\n<li>What is Gaussian Naive Bayes algorithm? What variants can be possible or why Gaussian?</li>\n</ol>\n<h2>Conclusion</h2>\n<p>In this article, we learned about the Naive Bayes classifier in detail. We drew the intuition from Baye's theorem of probability and stated its correlation with the Naive Bayes algorithm. We also saw the python implementation of this algorithm using sklearn. In the last, we discussed the advantages, disadvantages, and industrial applications. We hope you enjoyed the article.</p></div>",
        "Description": "Naive Bayes is a popular supervised machine learning algorithm used to predict the categorical target variables",
        "AuthorAndReviewer": "Ravish Raj",
        "Tag": "MachineLearning",
        "DomainFlagTag": "MachineLearning",
        "coverImage": null
      }
    }
  ],
  "meta": {
    "pagination": {
      "page": 1,
      "pageSize": 25,
      "pageCount": 1,
      "total": 1
    }
  }
}