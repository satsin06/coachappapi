{
    "contents": [
        {
            "description": "K-Nearest Neighbor is a Supervised Machine Learning algorithm that can be used to solve both classification and regression problems. The explainability it brings is rare to find among other advanced algorithms.",
            "heading": "Print Matrix in Spiral Order",
            "Body": "<div class=\"mt-6 ml-0 max-w-none md:ml-10 prose sm:prose-lg xl:prose-xl select-none\"><p>K-Nearest Neighbor is a Supervised Machine Learning algorithm that can be used to solve classification as well as regression problems. It is probably the first \"machine learning\" algorithm developed, and because of its simple nature, it is still widely accepted in solving many industrial problems. The unique thing about this algorithm is it learns but without explicitly mapping input variables to the target variables. In this article, we are going to understand this algorithm in detail.</p>\n<h2>Key Takeaways from this&nbsp;blog:</h2>\n<p>After going through this article, we will understand the following things:</p>\n<ol>\n<li>What is the KNN algorithm in Machine Learning?</li>\n<li>Why is KNN instance-based learning or a Lazy learner?</li>\n<li>Why KNN is a non-parametric algorithm?</li>\n<li>What are the common assumptions in KNN?</li>\n<li>How does KNN work?</li>\n<li>How the value of K affects the KNN algorithm?</li>\n<li>How does feature scaling affect KNN?</li>\n<li>What are the Voronoi cell and Voronoi diagrams?</li>\n<li>KNN for regression problems.</li>\n<li>Implementation of the KNN algorithm in python.</li>\n</ol>\n<p>So let's start without any further delay.</p>\n<h2>What is the KNN algorithm in Machine Learning?</h2>\n<p>In the introduction section, we already have explained KNN formally. Now, let's understand it in layman's terms. Some friends did not understand the concepts in our school days and still scored well in exams because of their memorization skills. We can correlate those friends with KNN. This ML algorithm does not follow the traditional approach of learning parameters from the training data and tries to fit a function. Instead, it memorizes the complete training data instances, and whenever a new test sample comes, it tries to verify the similarity of the test sample with its learned training samples.</p>\n<h2>Why is KNN instance-based learning or a Lazy&nbsp;learner?</h2>\n<p>Instance-based learning is also known as memory-based learning<strong>.</strong> Instead of explicit generalization, KNN compares new data samples with training data samples present in its memory.</p>\n<p>They are also called lazy algorithms, as any computation only happens when we receive new observations. Before accepting any test sample, it just memorizes everything in its memory and defers the calculations for the last like a lazy person.</p>\n<h2>Why KNN is a non-parametric algorithm?</h2>\n<p>KNN comes under the <strong><a href=\"https://www.enjoyalgorithms.com/blog/classification-of-machine-learning-models\">non-parametric algorithm</a></strong> category. Can we guess why? It is learning the complete training set, so if there are more instances in the future, the learning will change drastically. Hence learning is not dependent on the given data, which is a characteristic of a non-parametric algorithm.</p>\n<h2>What are the common assumptions in&nbsp;KNN?</h2>\n<p>This algorithm makes two major assumptions,</p>\n<ul>\n<li>Every sample part of the training data is mapped to real n-dimensional space. We can say that every sample will have the same dimension or number of attributes in simple terms.&nbsp;</li>\n<li>The \"nearest neighbors\" are defined in terms of <strong>Euclidean Distance</strong>, <strong>Manhattan Distance</strong>, or <strong>Hamming Distance</strong>. The choice of distance matters a lot and can change the prediction.</li>\n</ul>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/0*MBTyUQ648MfodIMb.png\" alt=\"Distance metrics\" title=\"Distance metrics\"></p>\n<h2><strong>Working of&nbsp;KNN</strong></h2>\n<p>Let's understand the stepwise analysis of this algorithm for any classification problem.</p>\n<p><strong>Step1:</strong> We first need to select the number of neighbors we want to consider. This is the term K in the KNN algorithm and highly affects the prediction.</p>\n<p><strong>Step2: We</strong> need to find the K neighbors based on any distance metric. It can be Euclidean, Manhatten, or our custom distance metric. We will have the test sample on which we want the prediction. The Closest K samples in the training data from this test sample will be our K neighbors.</p>\n<p><strong>Step3:</strong> Among the selected K neighbors, we need to count how many neighbors are from the different classes.&nbsp;</p>\n<p><strong>Step4:</strong> Now, we have to assign the test data sample to the class for which the count of neighbors was maximum.</p>\n<p>We performed the prediction in these four simple steps. In summary, the KNN algorithm at the training phase stores the dataset, and when it gets a new query, it classifies that query into a class similar to the existing query.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*-S8Zf-MufUStk1OEyEzuFQ.jpeg\" alt=\"1-NN vs. 4-NN \" title=\"1-NN vs. 4-NN\"></p>\n<p>Consider an example shown in the above image. Initially, the entire training dataset is considered and mapped in an R² space of positive and negative classes. The test case <strong>xq</strong> is then classified using 1-NN (1 neighbor) and 4-NN (4 neighbors) classifiers. The results for both are different, as we see that <strong>xq</strong> is classified as <strong>+ve for 1-NN</strong> and <strong>-ve for 4-NN.</strong>&nbsp;</p>\n<h2>How the value of K affects the KNN algorithm?</h2>\n<p>The value of K in the KNN algorithm can be anything ranging from 1 to the total number of samples. A small value of K means that the model is overfitting and is vulnerable to outliers. This model will have high variance and low bias. On the other hand, a model with a high value of K will have low variance and high bias and will result in underfitting. When we slowly increase the value of K from 1 to the number of training samples, the model will start smoothing the boundary surfaces.</p>\n<p><strong>K = 1:</strong> A model with K=1 will have 0 training error and hard boundaries for determining the class of test query.</p>\n<p><strong>K = len(sample data):</strong> This model will be highly biased towards the majority class (with a higher number of samples) and less accurate.</p>\n<p><strong>Note:</strong> Keeping the K values as odd is advisable to reduce the chances of getting a tie.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*7yP2rK3WSNvw_rSXecvPnQ.jpeg\"></p>\n<h2>How does feature scaling affect&nbsp;KNN?</h2>\n<p>KNN depends highly on the distance between data samples; hence scaling plays a vital role here. Suppose we train the KNN algorithm on unscaled data. There can be a case where different attributes lie in various scales, making our model biased towards the features with lesser magnitude values. To avoid that, it is always advisable to standardize the attributes before applying the KNN algorithm. Please look at <a href=\"https://www.enjoyalgorithms.com/blog/need-of-feature-scaling-in-machine-learning\">this blog</a> to visualize how distance calculation can be affected by scaling for a better understanding.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/0*-tv0vjTEud6hDKWn.png\" alt=\"Source: Scikit-learn.org, Scaling affect KNN \" title=\"Scaling affect KNN \"></p>\n<h2>What are the Voronoi cell and Voronoi diagrams?</h2>\n<p>Other ML algorithms like linear regression, logistic regression, and SVMs try to fit a mapping function from input to output. This mapping function is also known as the <strong>Hypothesis function</strong>. But, KNN is different. It does not form any explicit Hypothesis function, but it does create a hypothesis space. For a dataset in R², the hypothesis space is a polyhedron formed using the training samples. Let's first understand what Voronoi cell is.</p>\n<h3><strong>What is Voronoi&nbsp;Cell?</strong></h3>\n<p>Suppose the training set is \"T\" and the elements of that training set are \"x\"<strong>.</strong> Then Voronoi Cell of <strong>xi</strong> is a polytope (a geometric shape with \"flat\" sides) consisting of all points closer to <strong>xi</strong> than any other points in <strong>T.</strong></p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*vvDVXsv2ulT_-IaikeAXNg.jpeg\" alt=\"Voronoi Cell and polytope\" title=\"Voronoi Cell and polytope\"></p>\n<p>If we observe in the above image, initially, every cell contains a single sample which means K = 0, and as we increase the value of K, two cells merge and form a new polytope including K samples. Voronoi Cells cover the entire training space of T, and when we combine all of these cells, it will create Voronoi Diagram.</p>\n<h2>KNN for Regression problems</h2>\n<p>So far, we have discussed how we could use the KNN algorithm to solve the classification tasks, but this machine learning algorithm can also solve regression problems. We need to tweak the approach slightly. Instead of counting the <strong>K</strong> nearest neighbor class labels, what if we average the data over K neighbors<strong><em>.</em></strong> Yes! It will act as the regression model in such a scenario.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*RWeDQNYq_K3bLqA6K3ni1A.jpeg\">k-NN for Regression tasks</p>\n<p>For example, let's say we have a test data X for which we want to predict the continuous variable Y. Suppose we have finalized that our neighbors can only be 3 (i.e., K=3). Three neighbors from the training data are:</p>\n<p>X1 → Y1, X2 → Y2, X3 → Y3. We should be clear that KNN is a supervised learning algorithm, and hence we will always have the corresponding labels for the input variables while training. At the time of prediction, we can average out the three labels to find the corresponding label of the test data. For example, Y = (Y1 + Y2 + Y3)/3. This averaging can be replaced with other techniques like median, mode, or any custom approach.&nbsp;</p>\n<h2>Strengths of KNN algorithm</h2>\n<p>KNN is a very famous algorithm because of its simplicity, so let's understand the key strengths.&nbsp;</p>\n<ol>\n<li><strong>Zero training time:</strong> A very little training time is required compared to the other machine learning algorithms.</li>\n<li><strong>Sample efficiency:</strong> There is no need for a very high training sample.</li>\n<li><strong>Explainable:</strong> At each step, the reason for the prediction can easily be depicted. Such explainability is rare.</li>\n<li><strong>Easy to add and remove the data:</strong> For other machine learning models, data addition requires retraining of the model. While in KNN, we can directly update the memory and perform the inference.</li>\n<li><strong>Less sensitive to class imbalance:</strong> Suppose we have two classes and one class has significantly higher instances in the dataset than others. KNN, unlike other ML algorithms, is least affected by such class imbalances.</li>\n</ol>\n<h2>Disadvantages of k-NN algorithm</h2>\n<p>No doubt, KNN is cool, but this algorithm has some limitations. It is not the first choice among Machine Learning experts, and the reasons are:</p>\n<ol>\n<li><strong>Needs a lot of storage:</strong> KNN stores the whole training data in its memory and performs inference based on that. It makes the algorithm unemployable on edge platforms.</li>\n<li><strong>Predictions are Slow:</strong> The time complexity of KNN is O(dN), where <strong>d</strong> is the dimension or number of features and <strong>N</strong> is the total number of samples. More the data more will be the prediction time.</li>\n<li><strong>Irrelevant features can fool the nearest neighbors.</strong></li>\n</ol>\n<h2><strong>KNN Implementation in Python using&nbsp;sklearn</strong></h2>\n<p>Too much theory! Let's implement the KNN algorithm in python to solve a classification problem.&nbsp;</p>\n<h3><strong>Step 1: Import the necessary dataset libraries.</strong></h3>\n<p>The dataset used to implement KNN is the famous Iris dataset imported from the Scikit-learn datasets as load_iris. Other libraries are imported for training, preprocessing, and evaluation.</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt   # update the plot \nfrom sklearn import datasets# read the data \nimport numpy as np #for arrays \nfrom sklearn.model_selection import train_test_split # split the data \nfrom sklearn.preprocessing import StandardScaler # scale the data \nfrom sklearn.neighbors import KNeighborsClassifier # the algorithm \n\nfrom sklearn.metrics import accuracy_score  #grade the results \nimport pandas as pd \n\niris = datasets.load_iris() # read the data \n\nX = iris.data[:]  # select the features to use \ny = iris.target   # select the classes\n\n\niris_dataframe = pd.DataFrame (data= np.c_[iris['data'], iris['target']],\n\n    columns= iris['feature_names'] + ['target'])\n\nplt.figure(2)\ngrr = pd.plotting.scatter_matrix(iris_dataframe,\n                                  c=iris[\"target\"], \n                                  figsize=(15, 15),\n                                  marker='o', \n                                  S=60,\n                                  alpha=.8)\nplt.show(2)\n</code></pre>\n<h3>Step 2: Understanding the&nbsp;data</h3>\n<p>This dataset has four variables: <strong>sepal length, sepal width, petal length, and petal width,</strong> describing iris plants of three types: <strong>Setosa, Versicolour,</strong> and <strong>Virginica</strong>. The dataset contains 150 observations, with each observation labeled as the actual type of the plant.&nbsp;</p>\n<h3>Step 3: Visualization</h3>\n<p>The dataset, which has four dimensions, is visualized pairwise to distinguish them. The pairwise scatter plot matrix of the iris data set helps visualize the relationship among multiple variables separately within subdivisions of the dataset. In the image below, <strong>violet</strong> color represents <strong>Setosa</strong>, <strong>green</strong> represents <strong>Versicolour</strong>, and <strong>yellow</strong> represents <strong>Virginica</strong>.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*L4b2d0tYFWeEkcDuajgZyw.jpeg\" alt=\"Pairwise comparison for different features\" title=\"Pairwise comparison for different features\"></p>\n<h3><strong>Step 4: Data Preprocessing</strong></h3>\n<p>The entire dataset is initially split into the training and testing part using the train<em>test</em>split function of Scikit-learn. A standard scaler is used in the next step, StandardScalar( ), to standardize the data (column-wise). When fit to a dataset, the function will transform the dataset to <strong>mean μ = 0</strong> and <strong>standard deviation σ = 1.</strong></p>\n<p>A dataset with having N samples and m features,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*L_Q44QKznAFWYejvWXyKoQ.png\" alt=\"Distance Calculation\" title=\"Distance Calculation\"></p>\n<p>Thus every data is then updated as,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*UkLkymXlo54_RbAseOBr1Q.png\" alt=\"Standardization\" title=\"Standardization\"></p>\n<pre><code class=\"language-python\">X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,random_state=0)\n\n\nSC = StandardScaler()# create the standard scaler \nsc.fit(X_train) # fit to the training data \nx_train_std = sc.transform(X_train) # transform the training data \nX_test_std = sc.transform(X_test) # same transformation on test data\n</code></pre>\n<h3><strong>Step 5: Model Fitting and Evaluation</strong></h3>\n<p>We will fit the KNN model for different values of K ranging between 1 to the number of samples in the testing dataset. The metric <strong>\"Minkowski\"</strong> along with <strong>p = 2</strong> represents the Euclidean distance in the R-space. The model will be fitted on different values of K and then is used to predict the output for a test sample size.</p>\n<pre><code class=\"language-python\">accuracyTest = {}; accuracy Train = {} \n\nfor k in range (len (y_test):\n\n    knn = KNeighborsClassifier(n_neighbors=k+1, p=2, metric='minkowski')\n    knn.fit(X_train_std,y_train)\n    y_pred = knn.predict(x_test_std) \n    y_train_pred = knn.predict(X_train_std) \n\n    if (k+1)%10==0:\n        print(10*'-')\n        print(\"For k = %s\" %(k+1))\n        print('Number in test ', len(y_test))\n        print('Misclassified samples: %d' % (y_test != y_pred).sum())\n\n    accTrain = accuracy_score(y_train,y_train_pred)\n    acc = accuracy_score(y_test, y_pred)\n    accuracyTest[k+1] = acc\n    accuracyTrain[k+1] = accTrain\n\nfor accuracy in [accuracy Train, accuracy Test]:\n    lists = sorted(accuracy.items() # sorted by key, return a list of tuples \n    X, y = zip(*lists) # unpack a list of pairs into two tuples \n    plt.plot(x, y)\n    plt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*2w5zsVeff7Ob1tMp4p8E2Q.png\" alt=\"Training and Testing accuracy\" title=\"Training and Testing accuracy\"></p>\n<p>If we give priority to the testing accuracy, the value of K &gt; 18 decreases the testing accuracy sharply. So we can say that the optimal number of neighbors can be around 15 to 18.</p>\n<h2><strong>Decision Boundaries for&nbsp;KNN</strong></h2>\n<p>The two datasets (training and testing) are combined to show the effect of varying K in the KNN algorithm. Only two features (petal length and petal width) are considered for visualization. The value of K taken is [1,25,50,75,100,112], where the training sample size is 112. The decision boundary at K = 112 returns the majority of the three classes, which is red.</p>\n<pre><code class=\"language-python\">X = iris.data[:, [2,3]] # select the features to use \ny = iris.target         # select the classes\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,random_state=0)\n\n\nSC = StandardScaler()# create the standard scaler \nsc.fit(X_train) # fit to the training data \nx_train_std = sc.transform(X_train) # transform the training data \nX_test_std = sc.transform(X_test) # same transformation on test data\n\nX_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))\n\nprint('Number in combined ', len(y_combined))\n# check results on combined data \ny_combined_pred = knn.predict(X_combined_std)\n\nprint('Misclassified combined samples: %d' 1 % (y_combined != y combined_pred). sum )\nprint('Combined Accuracy: %.2f' % accuracy_score(y_combined, y_combined_pred)) \n# visualize the results \n\nfor k in [1,25,50, 100, len(X_train)]:\n\n    knn = KNeighborsClassifier (n_neighbors=k, p=2, metric='minkowski')\n\n    knn.fit(X_train_std, y_train) \n\n    plot_decision_regions(X=X_combined_std, y=y_combined, classifier=knn,\n                                test_idx=range(105,150))\n\n    plt.xlabel('petal length [standardized]') \n    plt.ylabel('petal width [standardized]') \n    plt.title('k=%s'%k) \n    plt.legend(loc='upper left') \n    plt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*VXDGlFehAYqClSiAbsApSQ.jpeg\" alt=\"Decision boundary\" title=\"Decision boundary\"></p>\n<h2>Industrial Applications of&nbsp;KNN</h2>\n<p>Although there are certain limitations, this algorithm is widely used in industries because of its simplicity. Some of these applications are:</p>\n<ol>\n<li><strong>Email spam filtering:</strong> For detecting the trivial and fixed types of spam emails, KNN can perform well. The implementation steps of this algorithm can be found <a href=\"https://www.enjoyalgorithms.com/blog/email-spam-and-non-spam-filtering-using-machine-learning\">here</a>.</li>\n<li><strong>Wine Quality prediction:</strong> Wine quality prediction is a regression task and can be solved using the KNN algorithm. The implementation can be found <a href=\"https://www.enjoyalgorithms.com/blog/wine-quality-prediction\">here</a>.</li>\n<li><strong>Recommendation system:</strong> KNN is used to build the recommendation engines that recommend some products/movies/songs to the users based on their likings or disliking.&nbsp;</li>\n</ol>\n<h2>Possible Interview Questions</h2>\n<p>As we stated, this algorithm brings a lot of explainability with itself. Interviewers can ask more profound questions on this topic. Some of them could be,</p>\n<ol>\n<li>How k-NN is different from other Machine Learning algorithms?</li>\n<li>Will changing the distance metric affect the classification accuracy?</li>\n<li>Is k-NN highly sensitive to data normalization?&nbsp;</li>\n<li>Why is it a non-parametric algorithm?</li>\n<li>What are the major cons of the k-NN algorithm?</li>\n</ol>\n<h2>Conclusion</h2>\n<p>In this article, we have covered the concept of the first \"Machine Learning\" algorithm, i.e., KNearest Neighbour. We saw how we can define the instances as neighbors and how the value of K affects the predictions. We also discussed why feature scaling played a vital role and learned about the Voronoi Diagram. After that, we discussed the regression use-case of KNN. Finally, we implemented the KNN algorithm on the famous Iris dataset. We hope you enjoyed the article.</p>\n<h4>References</h4>\n<ol>\n<li><a href=\"http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\">Scikit-learn: Machine Learning in Python</a>, Pedregosa, <em>et al.</em>, JMLR 12, pp. 2825–2830, 2011</li>\n<li>Mitchell, T. M. (1997). Machine learning., McGraw Hill series in computer science New York: McGraw-Hill.</li>\n<li>UCI Machine Learning Repository: Iris Data Set.</li>\n<li>J. D. Hunter, “Matplotlib: A 2D Graphics Environment”, Computing in Science &amp; Engineering, vol. 9, no. 3, pp. 90–95, 2007.</li>\n</ol>\n<h4>Enjoy Learning! Enjoy Algorithms!</h4></div>",
        "createdAt": "2022-02-24T07:45:46.825Z",
        "updatedAt": "2022-02-26T19:27:02.372Z",
        "publishedAt": "2022-02-24T07:46:12.600Z"
        },
        {
            "description": "K-Nearest Neighbor is a Supervised Machine Learning algorithm that can be used to solve both classification and regression problems. The explainability it brings is rare to find among other advanced algorithms.",
            "heading": "Print Matrix in Spiral Order",
            "Body": "<div class=\"mt-6 ml-0 max-w-none md:ml-10 prose sm:prose-lg xl:prose-xl select-none\"><p>K-Nearest Neighbor is a Supervised Machine Learning algorithm that can be used to solve classification as well as regression problems. It is probably the first \"machine learning\" algorithm developed, and because of its simple nature, it is still widely accepted in solving many industrial problems. The unique thing about this algorithm is it learns but without explicitly mapping input variables to the target variables. In this article, we are going to understand this algorithm in detail.</p>\n<h2>Key Takeaways from this&nbsp;blog:</h2>\n<p>After going through this article, we will understand the following things:</p>\n<ol>\n<li>What is the KNN algorithm in Machine Learning?</li>\n<li>Why is KNN instance-based learning or a Lazy learner?</li>\n<li>Why KNN is a non-parametric algorithm?</li>\n<li>What are the common assumptions in KNN?</li>\n<li>How does KNN work?</li>\n<li>How the value of K affects the KNN algorithm?</li>\n<li>How does feature scaling affect KNN?</li>\n<li>What are the Voronoi cell and Voronoi diagrams?</li>\n<li>KNN for regression problems.</li>\n<li>Implementation of the KNN algorithm in python.</li>\n</ol>\n<p>So let's start without any further delay.</p>\n<h2>What is the KNN algorithm in Machine Learning?</h2>\n<p>In the introduction section, we already have explained KNN formally. Now, let's understand it in layman's terms. Some friends did not understand the concepts in our school days and still scored well in exams because of their memorization skills. We can correlate those friends with KNN. This ML algorithm does not follow the traditional approach of learning parameters from the training data and tries to fit a function. Instead, it memorizes the complete training data instances, and whenever a new test sample comes, it tries to verify the similarity of the test sample with its learned training samples.</p>\n<h2>Why is KNN instance-based learning or a Lazy&nbsp;learner?</h2>\n<p>Instance-based learning is also known as memory-based learning<strong>.</strong> Instead of explicit generalization, KNN compares new data samples with training data samples present in its memory.</p>\n<p>They are also called lazy algorithms, as any computation only happens when we receive new observations. Before accepting any test sample, it just memorizes everything in its memory and defers the calculations for the last like a lazy person.</p>\n<h2>Why KNN is a non-parametric algorithm?</h2>\n<p>KNN comes under the <strong><a href=\"https://www.enjoyalgorithms.com/blog/classification-of-machine-learning-models\">non-parametric algorithm</a></strong> category. Can we guess why? It is learning the complete training set, so if there are more instances in the future, the learning will change drastically. Hence learning is not dependent on the given data, which is a characteristic of a non-parametric algorithm.</p>\n<h2>What are the common assumptions in&nbsp;KNN?</h2>\n<p>This algorithm makes two major assumptions,</p>\n<ul>\n<li>Every sample part of the training data is mapped to real n-dimensional space. We can say that every sample will have the same dimension or number of attributes in simple terms.&nbsp;</li>\n<li>The \"nearest neighbors\" are defined in terms of <strong>Euclidean Distance</strong>, <strong>Manhattan Distance</strong>, or <strong>Hamming Distance</strong>. The choice of distance matters a lot and can change the prediction.</li>\n</ul>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/0*MBTyUQ648MfodIMb.png\" alt=\"Distance metrics\" title=\"Distance metrics\"></p>\n<h2><strong>Working of&nbsp;KNN</strong></h2>\n<p>Let's understand the stepwise analysis of this algorithm for any classification problem.</p>\n<p><strong>Step1:</strong> We first need to select the number of neighbors we want to consider. This is the term K in the KNN algorithm and highly affects the prediction.</p>\n<p><strong>Step2: We</strong> need to find the K neighbors based on any distance metric. It can be Euclidean, Manhatten, or our custom distance metric. We will have the test sample on which we want the prediction. The Closest K samples in the training data from this test sample will be our K neighbors.</p>\n<p><strong>Step3:</strong> Among the selected K neighbors, we need to count how many neighbors are from the different classes.&nbsp;</p>\n<p><strong>Step4:</strong> Now, we have to assign the test data sample to the class for which the count of neighbors was maximum.</p>\n<p>We performed the prediction in these four simple steps. In summary, the KNN algorithm at the training phase stores the dataset, and when it gets a new query, it classifies that query into a class similar to the existing query.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*-S8Zf-MufUStk1OEyEzuFQ.jpeg\" alt=\"1-NN vs. 4-NN \" title=\"1-NN vs. 4-NN\"></p>\n<p>Consider an example shown in the above image. Initially, the entire training dataset is considered and mapped in an R² space of positive and negative classes. The test case <strong>xq</strong> is then classified using 1-NN (1 neighbor) and 4-NN (4 neighbors) classifiers. The results for both are different, as we see that <strong>xq</strong> is classified as <strong>+ve for 1-NN</strong> and <strong>-ve for 4-NN.</strong>&nbsp;</p>\n<h2>How the value of K affects the KNN algorithm?</h2>\n<p>The value of K in the KNN algorithm can be anything ranging from 1 to the total number of samples. A small value of K means that the model is overfitting and is vulnerable to outliers. This model will have high variance and low bias. On the other hand, a model with a high value of K will have low variance and high bias and will result in underfitting. When we slowly increase the value of K from 1 to the number of training samples, the model will start smoothing the boundary surfaces.</p>\n<p><strong>K = 1:</strong> A model with K=1 will have 0 training error and hard boundaries for determining the class of test query.</p>\n<p><strong>K = len(sample data):</strong> This model will be highly biased towards the majority class (with a higher number of samples) and less accurate.</p>\n<p><strong>Note:</strong> Keeping the K values as odd is advisable to reduce the chances of getting a tie.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*7yP2rK3WSNvw_rSXecvPnQ.jpeg\"></p>\n<h2>How does feature scaling affect&nbsp;KNN?</h2>\n<p>KNN depends highly on the distance between data samples; hence scaling plays a vital role here. Suppose we train the KNN algorithm on unscaled data. There can be a case where different attributes lie in various scales, making our model biased towards the features with lesser magnitude values. To avoid that, it is always advisable to standardize the attributes before applying the KNN algorithm. Please look at <a href=\"https://www.enjoyalgorithms.com/blog/need-of-feature-scaling-in-machine-learning\">this blog</a> to visualize how distance calculation can be affected by scaling for a better understanding.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/0*-tv0vjTEud6hDKWn.png\" alt=\"Source: Scikit-learn.org, Scaling affect KNN \" title=\"Scaling affect KNN \"></p>\n<h2>What are the Voronoi cell and Voronoi diagrams?</h2>\n<p>Other ML algorithms like linear regression, logistic regression, and SVMs try to fit a mapping function from input to output. This mapping function is also known as the <strong>Hypothesis function</strong>. But, KNN is different. It does not form any explicit Hypothesis function, but it does create a hypothesis space. For a dataset in R², the hypothesis space is a polyhedron formed using the training samples. Let's first understand what Voronoi cell is.</p>\n<h3><strong>What is Voronoi&nbsp;Cell?</strong></h3>\n<p>Suppose the training set is \"T\" and the elements of that training set are \"x\"<strong>.</strong> Then Voronoi Cell of <strong>xi</strong> is a polytope (a geometric shape with \"flat\" sides) consisting of all points closer to <strong>xi</strong> than any other points in <strong>T.</strong></p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*vvDVXsv2ulT_-IaikeAXNg.jpeg\" alt=\"Voronoi Cell and polytope\" title=\"Voronoi Cell and polytope\"></p>\n<p>If we observe in the above image, initially, every cell contains a single sample which means K = 0, and as we increase the value of K, two cells merge and form a new polytope including K samples. Voronoi Cells cover the entire training space of T, and when we combine all of these cells, it will create Voronoi Diagram.</p>\n<h2>KNN for Regression problems</h2>\n<p>So far, we have discussed how we could use the KNN algorithm to solve the classification tasks, but this machine learning algorithm can also solve regression problems. We need to tweak the approach slightly. Instead of counting the <strong>K</strong> nearest neighbor class labels, what if we average the data over K neighbors<strong><em>.</em></strong> Yes! It will act as the regression model in such a scenario.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*RWeDQNYq_K3bLqA6K3ni1A.jpeg\">k-NN for Regression tasks</p>\n<p>For example, let's say we have a test data X for which we want to predict the continuous variable Y. Suppose we have finalized that our neighbors can only be 3 (i.e., K=3). Three neighbors from the training data are:</p>\n<p>X1 → Y1, X2 → Y2, X3 → Y3. We should be clear that KNN is a supervised learning algorithm, and hence we will always have the corresponding labels for the input variables while training. At the time of prediction, we can average out the three labels to find the corresponding label of the test data. For example, Y = (Y1 + Y2 + Y3)/3. This averaging can be replaced with other techniques like median, mode, or any custom approach.&nbsp;</p>\n<h2>Strengths of KNN algorithm</h2>\n<p>KNN is a very famous algorithm because of its simplicity, so let's understand the key strengths.&nbsp;</p>\n<ol>\n<li><strong>Zero training time:</strong> A very little training time is required compared to the other machine learning algorithms.</li>\n<li><strong>Sample efficiency:</strong> There is no need for a very high training sample.</li>\n<li><strong>Explainable:</strong> At each step, the reason for the prediction can easily be depicted. Such explainability is rare.</li>\n<li><strong>Easy to add and remove the data:</strong> For other machine learning models, data addition requires retraining of the model. While in KNN, we can directly update the memory and perform the inference.</li>\n<li><strong>Less sensitive to class imbalance:</strong> Suppose we have two classes and one class has significantly higher instances in the dataset than others. KNN, unlike other ML algorithms, is least affected by such class imbalances.</li>\n</ol>\n<h2>Disadvantages of k-NN algorithm</h2>\n<p>No doubt, KNN is cool, but this algorithm has some limitations. It is not the first choice among Machine Learning experts, and the reasons are:</p>\n<ol>\n<li><strong>Needs a lot of storage:</strong> KNN stores the whole training data in its memory and performs inference based on that. It makes the algorithm unemployable on edge platforms.</li>\n<li><strong>Predictions are Slow:</strong> The time complexity of KNN is O(dN), where <strong>d</strong> is the dimension or number of features and <strong>N</strong> is the total number of samples. More the data more will be the prediction time.</li>\n<li><strong>Irrelevant features can fool the nearest neighbors.</strong></li>\n</ol>\n<h2><strong>KNN Implementation in Python using&nbsp;sklearn</strong></h2>\n<p>Too much theory! Let's implement the KNN algorithm in python to solve a classification problem.&nbsp;</p>\n<h3><strong>Step 1: Import the necessary dataset libraries.</strong></h3>\n<p>The dataset used to implement KNN is the famous Iris dataset imported from the Scikit-learn datasets as load_iris. Other libraries are imported for training, preprocessing, and evaluation.</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt   # update the plot \nfrom sklearn import datasets# read the data \nimport numpy as np #for arrays \nfrom sklearn.model_selection import train_test_split # split the data \nfrom sklearn.preprocessing import StandardScaler # scale the data \nfrom sklearn.neighbors import KNeighborsClassifier # the algorithm \n\nfrom sklearn.metrics import accuracy_score  #grade the results \nimport pandas as pd \n\niris = datasets.load_iris() # read the data \n\nX = iris.data[:]  # select the features to use \ny = iris.target   # select the classes\n\n\niris_dataframe = pd.DataFrame (data= np.c_[iris['data'], iris['target']],\n\n    columns= iris['feature_names'] + ['target'])\n\nplt.figure(2)\ngrr = pd.plotting.scatter_matrix(iris_dataframe,\n                                  c=iris[\"target\"], \n                                  figsize=(15, 15),\n                                  marker='o', \n                                  S=60,\n                                  alpha=.8)\nplt.show(2)\n</code></pre>\n<h3>Step 2: Understanding the&nbsp;data</h3>\n<p>This dataset has four variables: <strong>sepal length, sepal width, petal length, and petal width,</strong> describing iris plants of three types: <strong>Setosa, Versicolour,</strong> and <strong>Virginica</strong>. The dataset contains 150 observations, with each observation labeled as the actual type of the plant.&nbsp;</p>\n<h3>Step 3: Visualization</h3>\n<p>The dataset, which has four dimensions, is visualized pairwise to distinguish them. The pairwise scatter plot matrix of the iris data set helps visualize the relationship among multiple variables separately within subdivisions of the dataset. In the image below, <strong>violet</strong> color represents <strong>Setosa</strong>, <strong>green</strong> represents <strong>Versicolour</strong>, and <strong>yellow</strong> represents <strong>Virginica</strong>.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*L4b2d0tYFWeEkcDuajgZyw.jpeg\" alt=\"Pairwise comparison for different features\" title=\"Pairwise comparison for different features\"></p>\n<h3><strong>Step 4: Data Preprocessing</strong></h3>\n<p>The entire dataset is initially split into the training and testing part using the train<em>test</em>split function of Scikit-learn. A standard scaler is used in the next step, StandardScalar( ), to standardize the data (column-wise). When fit to a dataset, the function will transform the dataset to <strong>mean μ = 0</strong> and <strong>standard deviation σ = 1.</strong></p>\n<p>A dataset with having N samples and m features,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*L_Q44QKznAFWYejvWXyKoQ.png\" alt=\"Distance Calculation\" title=\"Distance Calculation\"></p>\n<p>Thus every data is then updated as,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*UkLkymXlo54_RbAseOBr1Q.png\" alt=\"Standardization\" title=\"Standardization\"></p>\n<pre><code class=\"language-python\">X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,random_state=0)\n\n\nSC = StandardScaler()# create the standard scaler \nsc.fit(X_train) # fit to the training data \nx_train_std = sc.transform(X_train) # transform the training data \nX_test_std = sc.transform(X_test) # same transformation on test data\n</code></pre>\n<h3><strong>Step 5: Model Fitting and Evaluation</strong></h3>\n<p>We will fit the KNN model for different values of K ranging between 1 to the number of samples in the testing dataset. The metric <strong>\"Minkowski\"</strong> along with <strong>p = 2</strong> represents the Euclidean distance in the R-space. The model will be fitted on different values of K and then is used to predict the output for a test sample size.</p>\n<pre><code class=\"language-python\">accuracyTest = {}; accuracy Train = {} \n\nfor k in range (len (y_test):\n\n    knn = KNeighborsClassifier(n_neighbors=k+1, p=2, metric='minkowski')\n    knn.fit(X_train_std,y_train)\n    y_pred = knn.predict(x_test_std) \n    y_train_pred = knn.predict(X_train_std) \n\n    if (k+1)%10==0:\n        print(10*'-')\n        print(\"For k = %s\" %(k+1))\n        print('Number in test ', len(y_test))\n        print('Misclassified samples: %d' % (y_test != y_pred).sum())\n\n    accTrain = accuracy_score(y_train,y_train_pred)\n    acc = accuracy_score(y_test, y_pred)\n    accuracyTest[k+1] = acc\n    accuracyTrain[k+1] = accTrain\n\nfor accuracy in [accuracy Train, accuracy Test]:\n    lists = sorted(accuracy.items() # sorted by key, return a list of tuples \n    X, y = zip(*lists) # unpack a list of pairs into two tuples \n    plt.plot(x, y)\n    plt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*2w5zsVeff7Ob1tMp4p8E2Q.png\" alt=\"Training and Testing accuracy\" title=\"Training and Testing accuracy\"></p>\n<p>If we give priority to the testing accuracy, the value of K &gt; 18 decreases the testing accuracy sharply. So we can say that the optimal number of neighbors can be around 15 to 18.</p>\n<h2><strong>Decision Boundaries for&nbsp;KNN</strong></h2>\n<p>The two datasets (training and testing) are combined to show the effect of varying K in the KNN algorithm. Only two features (petal length and petal width) are considered for visualization. The value of K taken is [1,25,50,75,100,112], where the training sample size is 112. The decision boundary at K = 112 returns the majority of the three classes, which is red.</p>\n<pre><code class=\"language-python\">X = iris.data[:, [2,3]] # select the features to use \ny = iris.target         # select the classes\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,random_state=0)\n\n\nSC = StandardScaler()# create the standard scaler \nsc.fit(X_train) # fit to the training data \nx_train_std = sc.transform(X_train) # transform the training data \nX_test_std = sc.transform(X_test) # same transformation on test data\n\nX_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))\n\nprint('Number in combined ', len(y_combined))\n# check results on combined data \ny_combined_pred = knn.predict(X_combined_std)\n\nprint('Misclassified combined samples: %d' 1 % (y_combined != y combined_pred). sum )\nprint('Combined Accuracy: %.2f' % accuracy_score(y_combined, y_combined_pred)) \n# visualize the results \n\nfor k in [1,25,50, 100, len(X_train)]:\n\n    knn = KNeighborsClassifier (n_neighbors=k, p=2, metric='minkowski')\n\n    knn.fit(X_train_std, y_train) \n\n    plot_decision_regions(X=X_combined_std, y=y_combined, classifier=knn,\n                                test_idx=range(105,150))\n\n    plt.xlabel('petal length [standardized]') \n    plt.ylabel('petal width [standardized]') \n    plt.title('k=%s'%k) \n    plt.legend(loc='upper left') \n    plt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*VXDGlFehAYqClSiAbsApSQ.jpeg\" alt=\"Decision boundary\" title=\"Decision boundary\"></p>\n<h2>Industrial Applications of&nbsp;KNN</h2>\n<p>Although there are certain limitations, this algorithm is widely used in industries because of its simplicity. Some of these applications are:</p>\n<ol>\n<li><strong>Email spam filtering:</strong> For detecting the trivial and fixed types of spam emails, KNN can perform well. The implementation steps of this algorithm can be found <a href=\"https://www.enjoyalgorithms.com/blog/email-spam-and-non-spam-filtering-using-machine-learning\">here</a>.</li>\n<li><strong>Wine Quality prediction:</strong> Wine quality prediction is a regression task and can be solved using the KNN algorithm. The implementation can be found <a href=\"https://www.enjoyalgorithms.com/blog/wine-quality-prediction\">here</a>.</li>\n<li><strong>Recommendation system:</strong> KNN is used to build the recommendation engines that recommend some products/movies/songs to the users based on their likings or disliking.&nbsp;</li>\n</ol>\n<h2>Possible Interview Questions</h2>\n<p>As we stated, this algorithm brings a lot of explainability with itself. Interviewers can ask more profound questions on this topic. Some of them could be,</p>\n<ol>\n<li>How k-NN is different from other Machine Learning algorithms?</li>\n<li>Will changing the distance metric affect the classification accuracy?</li>\n<li>Is k-NN highly sensitive to data normalization?&nbsp;</li>\n<li>Why is it a non-parametric algorithm?</li>\n<li>What are the major cons of the k-NN algorithm?</li>\n</ol>\n<h2>Conclusion</h2>\n<p>In this article, we have covered the concept of the first \"Machine Learning\" algorithm, i.e., KNearest Neighbour. We saw how we can define the instances as neighbors and how the value of K affects the predictions. We also discussed why feature scaling played a vital role and learned about the Voronoi Diagram. After that, we discussed the regression use-case of KNN. Finally, we implemented the KNN algorithm on the famous Iris dataset. We hope you enjoyed the article.</p>\n<h4>References</h4>\n<ol>\n<li><a href=\"http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\">Scikit-learn: Machine Learning in Python</a>, Pedregosa, <em>et al.</em>, JMLR 12, pp. 2825–2830, 2011</li>\n<li>Mitchell, T. M. (1997). Machine learning., McGraw Hill series in computer science New York: McGraw-Hill.</li>\n<li>UCI Machine Learning Repository: Iris Data Set.</li>\n<li>J. D. Hunter, “Matplotlib: A 2D Graphics Environment”, Computing in Science &amp; Engineering, vol. 9, no. 3, pp. 90–95, 2007.</li>\n</ol>\n<h4>Enjoy Learning! Enjoy Algorithms!</h4></div>",
        "createdAt": "2022-02-24T07:45:46.825Z",
        "updatedAt": "2022-02-26T19:27:02.372Z",
        "publishedAt": "2022-02-24T07:46:12.600Z"
        },
        {
            "description": "K-Nearest Neighbor is a Supervised Machine Learning algorithm that can be used to solve both classification and regression problems. The explainability it brings is rare to find among other advanced algorithms.",
            "heading": "Print Matrix in Spiral Order",
            "Body": "<div class=\"mt-6 ml-0 max-w-none md:ml-10 prose sm:prose-lg xl:prose-xl select-none\"><p>K-Nearest Neighbor is a Supervised Machine Learning algorithm that can be used to solve classification as well as regression problems. It is probably the first \"machine learning\" algorithm developed, and because of its simple nature, it is still widely accepted in solving many industrial problems. The unique thing about this algorithm is it learns but without explicitly mapping input variables to the target variables. In this article, we are going to understand this algorithm in detail.</p>\n<h2>Key Takeaways from this&nbsp;blog:</h2>\n<p>After going through this article, we will understand the following things:</p>\n<ol>\n<li>What is the KNN algorithm in Machine Learning?</li>\n<li>Why is KNN instance-based learning or a Lazy learner?</li>\n<li>Why KNN is a non-parametric algorithm?</li>\n<li>What are the common assumptions in KNN?</li>\n<li>How does KNN work?</li>\n<li>How the value of K affects the KNN algorithm?</li>\n<li>How does feature scaling affect KNN?</li>\n<li>What are the Voronoi cell and Voronoi diagrams?</li>\n<li>KNN for regression problems.</li>\n<li>Implementation of the KNN algorithm in python.</li>\n</ol>\n<p>So let's start without any further delay.</p>\n<h2>What is the KNN algorithm in Machine Learning?</h2>\n<p>In the introduction section, we already have explained KNN formally. Now, let's understand it in layman's terms. Some friends did not understand the concepts in our school days and still scored well in exams because of their memorization skills. We can correlate those friends with KNN. This ML algorithm does not follow the traditional approach of learning parameters from the training data and tries to fit a function. Instead, it memorizes the complete training data instances, and whenever a new test sample comes, it tries to verify the similarity of the test sample with its learned training samples.</p>\n<h2>Why is KNN instance-based learning or a Lazy&nbsp;learner?</h2>\n<p>Instance-based learning is also known as memory-based learning<strong>.</strong> Instead of explicit generalization, KNN compares new data samples with training data samples present in its memory.</p>\n<p>They are also called lazy algorithms, as any computation only happens when we receive new observations. Before accepting any test sample, it just memorizes everything in its memory and defers the calculations for the last like a lazy person.</p>\n<h2>Why KNN is a non-parametric algorithm?</h2>\n<p>KNN comes under the <strong><a href=\"https://www.enjoyalgorithms.com/blog/classification-of-machine-learning-models\">non-parametric algorithm</a></strong> category. Can we guess why? It is learning the complete training set, so if there are more instances in the future, the learning will change drastically. Hence learning is not dependent on the given data, which is a characteristic of a non-parametric algorithm.</p>\n<h2>What are the common assumptions in&nbsp;KNN?</h2>\n<p>This algorithm makes two major assumptions,</p>\n<ul>\n<li>Every sample part of the training data is mapped to real n-dimensional space. We can say that every sample will have the same dimension or number of attributes in simple terms.&nbsp;</li>\n<li>The \"nearest neighbors\" are defined in terms of <strong>Euclidean Distance</strong>, <strong>Manhattan Distance</strong>, or <strong>Hamming Distance</strong>. The choice of distance matters a lot and can change the prediction.</li>\n</ul>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/0*MBTyUQ648MfodIMb.png\" alt=\"Distance metrics\" title=\"Distance metrics\"></p>\n<h2><strong>Working of&nbsp;KNN</strong></h2>\n<p>Let's understand the stepwise analysis of this algorithm for any classification problem.</p>\n<p><strong>Step1:</strong> We first need to select the number of neighbors we want to consider. This is the term K in the KNN algorithm and highly affects the prediction.</p>\n<p><strong>Step2: We</strong> need to find the K neighbors based on any distance metric. It can be Euclidean, Manhatten, or our custom distance metric. We will have the test sample on which we want the prediction. The Closest K samples in the training data from this test sample will be our K neighbors.</p>\n<p><strong>Step3:</strong> Among the selected K neighbors, we need to count how many neighbors are from the different classes.&nbsp;</p>\n<p><strong>Step4:</strong> Now, we have to assign the test data sample to the class for which the count of neighbors was maximum.</p>\n<p>We performed the prediction in these four simple steps. In summary, the KNN algorithm at the training phase stores the dataset, and when it gets a new query, it classifies that query into a class similar to the existing query.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*-S8Zf-MufUStk1OEyEzuFQ.jpeg\" alt=\"1-NN vs. 4-NN \" title=\"1-NN vs. 4-NN\"></p>\n<p>Consider an example shown in the above image. Initially, the entire training dataset is considered and mapped in an R² space of positive and negative classes. The test case <strong>xq</strong> is then classified using 1-NN (1 neighbor) and 4-NN (4 neighbors) classifiers. The results for both are different, as we see that <strong>xq</strong> is classified as <strong>+ve for 1-NN</strong> and <strong>-ve for 4-NN.</strong>&nbsp;</p>\n<h2>How the value of K affects the KNN algorithm?</h2>\n<p>The value of K in the KNN algorithm can be anything ranging from 1 to the total number of samples. A small value of K means that the model is overfitting and is vulnerable to outliers. This model will have high variance and low bias. On the other hand, a model with a high value of K will have low variance and high bias and will result in underfitting. When we slowly increase the value of K from 1 to the number of training samples, the model will start smoothing the boundary surfaces.</p>\n<p><strong>K = 1:</strong> A model with K=1 will have 0 training error and hard boundaries for determining the class of test query.</p>\n<p><strong>K = len(sample data):</strong> This model will be highly biased towards the majority class (with a higher number of samples) and less accurate.</p>\n<p><strong>Note:</strong> Keeping the K values as odd is advisable to reduce the chances of getting a tie.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*7yP2rK3WSNvw_rSXecvPnQ.jpeg\"></p>\n<h2>How does feature scaling affect&nbsp;KNN?</h2>\n<p>KNN depends highly on the distance between data samples; hence scaling plays a vital role here. Suppose we train the KNN algorithm on unscaled data. There can be a case where different attributes lie in various scales, making our model biased towards the features with lesser magnitude values. To avoid that, it is always advisable to standardize the attributes before applying the KNN algorithm. Please look at <a href=\"https://www.enjoyalgorithms.com/blog/need-of-feature-scaling-in-machine-learning\">this blog</a> to visualize how distance calculation can be affected by scaling for a better understanding.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/0*-tv0vjTEud6hDKWn.png\" alt=\"Source: Scikit-learn.org, Scaling affect KNN \" title=\"Scaling affect KNN \"></p>\n<h2>What are the Voronoi cell and Voronoi diagrams?</h2>\n<p>Other ML algorithms like linear regression, logistic regression, and SVMs try to fit a mapping function from input to output. This mapping function is also known as the <strong>Hypothesis function</strong>. But, KNN is different. It does not form any explicit Hypothesis function, but it does create a hypothesis space. For a dataset in R², the hypothesis space is a polyhedron formed using the training samples. Let's first understand what Voronoi cell is.</p>\n<h3><strong>What is Voronoi&nbsp;Cell?</strong></h3>\n<p>Suppose the training set is \"T\" and the elements of that training set are \"x\"<strong>.</strong> Then Voronoi Cell of <strong>xi</strong> is a polytope (a geometric shape with \"flat\" sides) consisting of all points closer to <strong>xi</strong> than any other points in <strong>T.</strong></p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*vvDVXsv2ulT_-IaikeAXNg.jpeg\" alt=\"Voronoi Cell and polytope\" title=\"Voronoi Cell and polytope\"></p>\n<p>If we observe in the above image, initially, every cell contains a single sample which means K = 0, and as we increase the value of K, two cells merge and form a new polytope including K samples. Voronoi Cells cover the entire training space of T, and when we combine all of these cells, it will create Voronoi Diagram.</p>\n<h2>KNN for Regression problems</h2>\n<p>So far, we have discussed how we could use the KNN algorithm to solve the classification tasks, but this machine learning algorithm can also solve regression problems. We need to tweak the approach slightly. Instead of counting the <strong>K</strong> nearest neighbor class labels, what if we average the data over K neighbors<strong><em>.</em></strong> Yes! It will act as the regression model in such a scenario.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*RWeDQNYq_K3bLqA6K3ni1A.jpeg\">k-NN for Regression tasks</p>\n<p>For example, let's say we have a test data X for which we want to predict the continuous variable Y. Suppose we have finalized that our neighbors can only be 3 (i.e., K=3). Three neighbors from the training data are:</p>\n<p>X1 → Y1, X2 → Y2, X3 → Y3. We should be clear that KNN is a supervised learning algorithm, and hence we will always have the corresponding labels for the input variables while training. At the time of prediction, we can average out the three labels to find the corresponding label of the test data. For example, Y = (Y1 + Y2 + Y3)/3. This averaging can be replaced with other techniques like median, mode, or any custom approach.&nbsp;</p>\n<h2>Strengths of KNN algorithm</h2>\n<p>KNN is a very famous algorithm because of its simplicity, so let's understand the key strengths.&nbsp;</p>\n<ol>\n<li><strong>Zero training time:</strong> A very little training time is required compared to the other machine learning algorithms.</li>\n<li><strong>Sample efficiency:</strong> There is no need for a very high training sample.</li>\n<li><strong>Explainable:</strong> At each step, the reason for the prediction can easily be depicted. Such explainability is rare.</li>\n<li><strong>Easy to add and remove the data:</strong> For other machine learning models, data addition requires retraining of the model. While in KNN, we can directly update the memory and perform the inference.</li>\n<li><strong>Less sensitive to class imbalance:</strong> Suppose we have two classes and one class has significantly higher instances in the dataset than others. KNN, unlike other ML algorithms, is least affected by such class imbalances.</li>\n</ol>\n<h2>Disadvantages of k-NN algorithm</h2>\n<p>No doubt, KNN is cool, but this algorithm has some limitations. It is not the first choice among Machine Learning experts, and the reasons are:</p>\n<ol>\n<li><strong>Needs a lot of storage:</strong> KNN stores the whole training data in its memory and performs inference based on that. It makes the algorithm unemployable on edge platforms.</li>\n<li><strong>Predictions are Slow:</strong> The time complexity of KNN is O(dN), where <strong>d</strong> is the dimension or number of features and <strong>N</strong> is the total number of samples. More the data more will be the prediction time.</li>\n<li><strong>Irrelevant features can fool the nearest neighbors.</strong></li>\n</ol>\n<h2><strong>KNN Implementation in Python using&nbsp;sklearn</strong></h2>\n<p>Too much theory! Let's implement the KNN algorithm in python to solve a classification problem.&nbsp;</p>\n<h3><strong>Step 1: Import the necessary dataset libraries.</strong></h3>\n<p>The dataset used to implement KNN is the famous Iris dataset imported from the Scikit-learn datasets as load_iris. Other libraries are imported for training, preprocessing, and evaluation.</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt   # update the plot \nfrom sklearn import datasets# read the data \nimport numpy as np #for arrays \nfrom sklearn.model_selection import train_test_split # split the data \nfrom sklearn.preprocessing import StandardScaler # scale the data \nfrom sklearn.neighbors import KNeighborsClassifier # the algorithm \n\nfrom sklearn.metrics import accuracy_score  #grade the results \nimport pandas as pd \n\niris = datasets.load_iris() # read the data \n\nX = iris.data[:]  # select the features to use \ny = iris.target   # select the classes\n\n\niris_dataframe = pd.DataFrame (data= np.c_[iris['data'], iris['target']],\n\n    columns= iris['feature_names'] + ['target'])\n\nplt.figure(2)\ngrr = pd.plotting.scatter_matrix(iris_dataframe,\n                                  c=iris[\"target\"], \n                                  figsize=(15, 15),\n                                  marker='o', \n                                  S=60,\n                                  alpha=.8)\nplt.show(2)\n</code></pre>\n<h3>Step 2: Understanding the&nbsp;data</h3>\n<p>This dataset has four variables: <strong>sepal length, sepal width, petal length, and petal width,</strong> describing iris plants of three types: <strong>Setosa, Versicolour,</strong> and <strong>Virginica</strong>. The dataset contains 150 observations, with each observation labeled as the actual type of the plant.&nbsp;</p>\n<h3>Step 3: Visualization</h3>\n<p>The dataset, which has four dimensions, is visualized pairwise to distinguish them. The pairwise scatter plot matrix of the iris data set helps visualize the relationship among multiple variables separately within subdivisions of the dataset. In the image below, <strong>violet</strong> color represents <strong>Setosa</strong>, <strong>green</strong> represents <strong>Versicolour</strong>, and <strong>yellow</strong> represents <strong>Virginica</strong>.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*L4b2d0tYFWeEkcDuajgZyw.jpeg\" alt=\"Pairwise comparison for different features\" title=\"Pairwise comparison for different features\"></p>\n<h3><strong>Step 4: Data Preprocessing</strong></h3>\n<p>The entire dataset is initially split into the training and testing part using the train<em>test</em>split function of Scikit-learn. A standard scaler is used in the next step, StandardScalar( ), to standardize the data (column-wise). When fit to a dataset, the function will transform the dataset to <strong>mean μ = 0</strong> and <strong>standard deviation σ = 1.</strong></p>\n<p>A dataset with having N samples and m features,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*L_Q44QKznAFWYejvWXyKoQ.png\" alt=\"Distance Calculation\" title=\"Distance Calculation\"></p>\n<p>Thus every data is then updated as,</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*UkLkymXlo54_RbAseOBr1Q.png\" alt=\"Standardization\" title=\"Standardization\"></p>\n<pre><code class=\"language-python\">X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,random_state=0)\n\n\nSC = StandardScaler()# create the standard scaler \nsc.fit(X_train) # fit to the training data \nx_train_std = sc.transform(X_train) # transform the training data \nX_test_std = sc.transform(X_test) # same transformation on test data\n</code></pre>\n<h3><strong>Step 5: Model Fitting and Evaluation</strong></h3>\n<p>We will fit the KNN model for different values of K ranging between 1 to the number of samples in the testing dataset. The metric <strong>\"Minkowski\"</strong> along with <strong>p = 2</strong> represents the Euclidean distance in the R-space. The model will be fitted on different values of K and then is used to predict the output for a test sample size.</p>\n<pre><code class=\"language-python\">accuracyTest = {}; accuracy Train = {} \n\nfor k in range (len (y_test):\n\n    knn = KNeighborsClassifier(n_neighbors=k+1, p=2, metric='minkowski')\n    knn.fit(X_train_std,y_train)\n    y_pred = knn.predict(x_test_std) \n    y_train_pred = knn.predict(X_train_std) \n\n    if (k+1)%10==0:\n        print(10*'-')\n        print(\"For k = %s\" %(k+1))\n        print('Number in test ', len(y_test))\n        print('Misclassified samples: %d' % (y_test != y_pred).sum())\n\n    accTrain = accuracy_score(y_train,y_train_pred)\n    acc = accuracy_score(y_test, y_pred)\n    accuracyTest[k+1] = acc\n    accuracyTrain[k+1] = accTrain\n\nfor accuracy in [accuracy Train, accuracy Test]:\n    lists = sorted(accuracy.items() # sorted by key, return a list of tuples \n    X, y = zip(*lists) # unpack a list of pairs into two tuples \n    plt.plot(x, y)\n    plt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*2w5zsVeff7Ob1tMp4p8E2Q.png\" alt=\"Training and Testing accuracy\" title=\"Training and Testing accuracy\"></p>\n<p>If we give priority to the testing accuracy, the value of K &gt; 18 decreases the testing accuracy sharply. So we can say that the optimal number of neighbors can be around 15 to 18.</p>\n<h2><strong>Decision Boundaries for&nbsp;KNN</strong></h2>\n<p>The two datasets (training and testing) are combined to show the effect of varying K in the KNN algorithm. Only two features (petal length and petal width) are considered for visualization. The value of K taken is [1,25,50,75,100,112], where the training sample size is 112. The decision boundary at K = 112 returns the majority of the three classes, which is red.</p>\n<pre><code class=\"language-python\">X = iris.data[:, [2,3]] # select the features to use \ny = iris.target         # select the classes\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,random_state=0)\n\n\nSC = StandardScaler()# create the standard scaler \nsc.fit(X_train) # fit to the training data \nx_train_std = sc.transform(X_train) # transform the training data \nX_test_std = sc.transform(X_test) # same transformation on test data\n\nX_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))\n\nprint('Number in combined ', len(y_combined))\n# check results on combined data \ny_combined_pred = knn.predict(X_combined_std)\n\nprint('Misclassified combined samples: %d' 1 % (y_combined != y combined_pred). sum )\nprint('Combined Accuracy: %.2f' % accuracy_score(y_combined, y_combined_pred)) \n# visualize the results \n\nfor k in [1,25,50, 100, len(X_train)]:\n\n    knn = KNeighborsClassifier (n_neighbors=k, p=2, metric='minkowski')\n\n    knn.fit(X_train_std, y_train) \n\n    plot_decision_regions(X=X_combined_std, y=y_combined, classifier=knn,\n                                test_idx=range(105,150))\n\n    plt.xlabel('petal length [standardized]') \n    plt.ylabel('petal width [standardized]') \n    plt.title('k=%s'%k) \n    plt.legend(loc='upper left') \n    plt.show()\n</code></pre>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*VXDGlFehAYqClSiAbsApSQ.jpeg\" alt=\"Decision boundary\" title=\"Decision boundary\"></p>\n<h2>Industrial Applications of&nbsp;KNN</h2>\n<p>Although there are certain limitations, this algorithm is widely used in industries because of its simplicity. Some of these applications are:</p>\n<ol>\n<li><strong>Email spam filtering:</strong> For detecting the trivial and fixed types of spam emails, KNN can perform well. The implementation steps of this algorithm can be found <a href=\"https://www.enjoyalgorithms.com/blog/email-spam-and-non-spam-filtering-using-machine-learning\">here</a>.</li>\n<li><strong>Wine Quality prediction:</strong> Wine quality prediction is a regression task and can be solved using the KNN algorithm. The implementation can be found <a href=\"https://www.enjoyalgorithms.com/blog/wine-quality-prediction\">here</a>.</li>\n<li><strong>Recommendation system:</strong> KNN is used to build the recommendation engines that recommend some products/movies/songs to the users based on their likings or disliking.&nbsp;</li>\n</ol>\n<h2>Possible Interview Questions</h2>\n<p>As we stated, this algorithm brings a lot of explainability with itself. Interviewers can ask more profound questions on this topic. Some of them could be,</p>\n<ol>\n<li>How k-NN is different from other Machine Learning algorithms?</li>\n<li>Will changing the distance metric affect the classification accuracy?</li>\n<li>Is k-NN highly sensitive to data normalization?&nbsp;</li>\n<li>Why is it a non-parametric algorithm?</li>\n<li>What are the major cons of the k-NN algorithm?</li>\n</ol>\n<h2>Conclusion</h2>\n<p>In this article, we have covered the concept of the first \"Machine Learning\" algorithm, i.e., KNearest Neighbour. We saw how we can define the instances as neighbors and how the value of K affects the predictions. We also discussed why feature scaling played a vital role and learned about the Voronoi Diagram. After that, we discussed the regression use-case of KNN. Finally, we implemented the KNN algorithm on the famous Iris dataset. We hope you enjoyed the article.</p>\n<h4>References</h4>\n<ol>\n<li><a href=\"http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\">Scikit-learn: Machine Learning in Python</a>, Pedregosa, <em>et al.</em>, JMLR 12, pp. 2825–2830, 2011</li>\n<li>Mitchell, T. M. (1997). Machine learning., McGraw Hill series in computer science New York: McGraw-Hill.</li>\n<li>UCI Machine Learning Repository: Iris Data Set.</li>\n<li>J. D. Hunter, “Matplotlib: A 2D Graphics Environment”, Computing in Science &amp; Engineering, vol. 9, no. 3, pp. 90–95, 2007.</li>\n</ol>\n<h4>Enjoy Learning! Enjoy Algorithms!</h4></div>",
        "createdAt": "2022-02-24T07:45:46.825Z",
        "updatedAt": "2022-02-26T19:27:02.372Z",
        "publishedAt": "2022-02-24T07:46:12.600Z"
        },
        {
            "description": "The Builder is a creational design pattern that allows us to follow a step-by-step process to construct a complex object. Builder design pattern separates the construction of the complex object from its representation by which the same process can create different representations (types) of the complex object.",
            "heading": "Builder Design Pattern",
            "Body": "<div class=\"mt-6 ml-0 max-w-none md:ml-10 prose sm:prose-lg xl:prose-xl select-none\"><h3>What is Builder&nbsp;Pattern?</h3>\n<p>The Builder is a creational design pattern that allows us to follow a step-by-step process to construct a complex object. Builder design pattern separates the construction of the complex object from its representation by which the same process can create different representations (types) of the complex object.</p>\n<h3>Problem Statement</h3>\n<p>Let’s make some Pizzas!! We’ll understand the builder design pattern with an example of making pizzas.</p>\n<p>Making a pizza will consist of a recipe (series of steps). We might start with the dough, and then we’ll go to the pizza base, then the toppings followed by the sauce, and finally the baking part comes into play.</p>\n<h3>Solution Approach&nbsp;1</h3>\n<p>We can treat Pizza as a class with many fields like dough, pizza base, toppings, etc. Then we create a pizza using the constructor of the Pizza class.</p>\n<p><strong>Code</strong></p>\n<pre><code>//Pizza class with constructor\npublic class Pizza {\n    \n    //...\n    \n    private String dough;\n    private String base;\n    private String toppings;\n    private String sauce;\n    private String bake;\n    private String cheese;\n    public Pizza (String dough, String base, String toppings, \n                        String sauce, String bake, String cheese) \n    {\n        this.dough = dough;\n        this.base = base;\n        this.toppings =  toppings;\n        this.sauce = sauce;\n        this.bake = bake;\n        this.cheese = cheese;\n    }\n    \n    //...\n}\n</code></pre>\n<p>Well, that solves the problem, right? But the next consumer wants to have some extra toppings along with mozzarella cheese on it while another consumer wants it without any toppings. Now, we are forced to change our Pizza class and add a set of overloaded constructors, like this:</p>\n<pre><code>//set of overloaded constructors which are added in Pizza class\n\npublic Pizza(String dough, String base, String toppings,String sauce, String bake, String cheese)\n{\n    //...\n}\n    \npublic Pizza(String dough, String base, String sauce, String bake, String cheese)\n{\n    //pizza without toppings...  \n}\npublic Pizza(String dough, String base, String toppings, String sauce, String bake)\n{\n    //pizza without cheese...\n}\n</code></pre>\n<p><strong>Drawbacks</strong></p>\n<ul>\n<li>Technically, the set of overloaded constructors will give an error because the compiler will not be able to distinguish between the second and third constructors due to their same signature. Therefore, this is a crucial drawback: if the class fields have similar data types, we will have <strong>difficulty using constructor overloading</strong> for instantiation.</li>\n<li>As the number of combinations of parameters increases, the <strong>number of constructors will increase</strong>, suggesting that this approach is inefficient for complex and heavy classes (in terms of the class's number of parameters).</li>\n</ul>\n<p><strong>Takeaway:</strong> This particular approach is often termed a telescopic constructor pattern, which is, in fact, an <a href=\"https://en.wikipedia.org/wiki/Anti-pattern#:~:text=An%20anti%2Dpattern%20is%20a,and%20risks%20being%20highly%20counterproductive.\">anti-pattern</a>. So in place of using the telescopic constructor pattern, we should try a better approach.</p>\n<h3>Solution Approach&nbsp;2</h3>\n<p>To overcome the drawbacks in the telescopic constructor pattern, we can <strong>write setter methods for each field</strong> in the class and initialize them.&nbsp;</p>\n<p><strong>Code</strong></p>\n<pre><code>//... \npublic void setdough (String dough) {\n    this.dough = dough;\n}\npublic void setbase (String base) {\n    this.base = base;\n }\npublic void setsauce (String sauce) {\n    this.sauce = sauce;\n}\n//and so on....\n//...\n</code></pre>\n<p><strong>Drawbacks</strong></p>\n<ul>\n<li>For this approach to work, <strong>consumers have to call</strong> setter methods from the consumer part of the code with appropriate parameters in the <strong>correct order</strong> to make a pizza. Imagine, if a consumer calls setsauce() before setdough(), then he won’t end up with a pizza! This can happen quite often because the consumer (client) might not be aware of the recipe for pizza.</li>\n<li>Moreover, if a consumer shows up and says “I want a large size Mexican pizza”. Then the consumer part of the code has to call all the setters with new values which will be tedious.</li>\n</ul>\n<p><strong>Takeaway</strong></p>\n<ul>\n<li>The order in which the steps are performed is also our concern so, we should not give this responsibility to the consumer/client code.</li>\n<li>We should try to remove the responsibility of calling the setter methods from the <strong>consumer code</strong> because we may have a number of varieties of pizzas that require each step to work differently.</li>\n</ul>\n<p>The Builder pattern helps in solving such issues while creating a complex product.</p>\n<h3>Efficient Solution&nbsp;Approach</h3>\n<p>We know that all pizzas have to follow the same procedure (recipe) but the implementation of the steps might be different. For instance, an Italian pizza has different toppings, cheese from that of a Mexican pizza but, the steps performed in making both pizzas are the same. Therefore, we will first try to separate the <strong>algorithm of making the pizza</strong> that is, the recipe part from <strong>how a pizza is created and represented or,</strong> how the steps of the recipe are implemented and kept as the final product.&nbsp;</p>\n<p>So, we will hire a <strong>HeadChef</strong> who knows the recipe and we will also hire some talented <strong>Cooks</strong> who are specialized in making a certain type of pizza. For instance, <strong>ItalianCook</strong> knows how to make an <strong>Italian</strong> pizza whereas <strong>MexicanCook</strong> knows how to make a <strong>Mexican</strong> pizza. The role of HeadChef is to define the recipe to a Cook and then the other Cook will follow the recipe and finally give back the pizza to HeadChef.</p>\n<p>Notice that each Cook is supposed to follow the company's policy in making a pizza that is, they are only allowed to use a common interface of steps but they have to implement the steps as per the requirement of their pizza. For instance, <em>MexicanCook</em> can not add a new component in the Pizza class by himself. To solve these issues, we will introduce an interface named <strong>Cook</strong> that declares all the possible steps that can be involved in pizza making.</p>\n<p>In terms of classes and interfaces, the <em>HeadChef</em> will be a class that <strong>defines the steps</strong> in the correct order. <em>Cook</em> is an <strong>interface that consists of the methods</strong> to set the fields required in the <em>Pizza</em> product (e.g. dough, sauce, etc). All the <strong>sub-classes of <em>Cook</em></strong> (<em>MexicanCook and ItalianCook</em>) <strong>implement the methods</strong> provided in the <em>Cook</em>. Finally, the complete product (pizza) is returned by one of the concrete subclasses of Cook.</p>\n<p><strong>Takeaway</strong></p>\n<ul>\n<li>We have abstracted two different responsibilities:<br>\n— Order of construction steps (recipe).<br>\n— Ingredients/style of construction in each step.</li>\n<li>We will be using the same construction process (which is provided by <em>HeadChef</em>) to create different representations of our object (the <em>Pizza</em>).</li>\n<li>Also, the <em>Cook</em> interface hides the details of the construction process from the consumer. Therefore, now the consumer needs to associate a specific Cook with the <em>HeadChef</em> (he or she can do so by ordering a specific pizza) and he or she will get their order ready.</li>\n<li>Adding new types of Cooks is simple because we only have to make a new subclass of <em>Cook</em> and implement it. Whereas, in the previous approach, we had to change the existing consumer code.&nbsp;</li>\n</ul>\n<p>The final approach that we just discussed is <strong>the Builder pattern.</strong></p>\n<p>*NOTE: We can make more classes like HeadChef say PrimeChef, which might have a different construction process as per the need of the company, or we can define a new construction process in the HeadChef class itself. Then, we can use PrimeChef class with existing Cooks to get new desirable pizzas! In our example, <strong>we’ve considered only a single construction process</strong>, but there can be many construction processes with minute differences. For instance, maybe a super-fast variant of pizza has some other steps as compared to a general pizza.*</p>\n<h3>Components and Structure</h3>\n<p><img src=\"https://cdn-images-1.medium.com/max/640/1*4hE5PI4XuHvqeAM6dLm88A.png\"></p>\n<ol>\n<li><strong>Builder</strong> *(Cook)&nbsp;*An interface that declares steps of product construction common to all of its subclasses.</li>\n<li><strong>Concrete Builders</strong> (<em>ItalianCook, MexicanCook)</em>Implement methods of Builder class differently as per the demand of consumers. Different concrete builders provide different implementations so that, we can get different representations of the complex product.</li>\n<li><strong>Director</strong> <em>(HeadChef)</em>Defines the appropriate order in which all the construction steps have to be invoked. Uses the Builder interface to create an instance of the complex product.</li>\n<li><strong>Product</strong> <em>(Pizza)</em>The final objects are returned by the concrete builders. Again, we emphasize that all the products need not belong to the same base class, although in this example we have taken all the products to belong to the same class <strong>Pizza</strong> but the products can belong to different classes. In such a case, the consumer code directly calls the method of Concrete builders which return the final product. (We’ll look at a small example later)</li>\n<li><strong>Client</strong> <em>(Consumer)</em>This part of the code associates one of the concrete builders with the director. Then, constructs the product via the director class.</li>\n</ol>\n<h3>Implementation</h3>\n<p>We’ve already explored the problem and various possible solutions, among which the first two approaches were inefficient and the last approach is the builder pattern which overcomes the drawbacks of the rest of the approaches up to great extent.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/640/1*Sr97jvStXGDahJ2cfGnwKw.png\"></p>\n<h4>Implementation Approach&nbsp;(Java)</h4>\n<p>Firstly, we create the product class — <strong><em>Pizza,</em></strong> which consists of various fields and setter methods of those fields (e.g, setdough(), setbase(), etc). Notice that these parts can be objects of some other class also.</p>\n<pre><code>public static class Pizza {\n    private String dough;\n    private String base;\n    private String toppings;\n    private String sauce;\n    private String bake;\n    private String cheese;\n        \n    public void setdough (String dough) {\n        this.dough = dough;\n    }\n    public void setbase (String base) {\n        this.base = base;\n    }\n    public void settoppings (String toppings) {\n        this.toppings = toppings;\n    }\n    public void setsauce (String sauce) {\n        this.sauce = sauce;\n    }\n    public void setbake (String bake) {\n        this.bake = bake;\n    }\n    public void setcheese (String cheese) {\n        this.cheese = cheese;\n    }\n    public void ShowPizza () {\n        System.out.println(dough+\", \"+base+\", \"+toppings+\",   \"+sauce+\", \"+bake+\", \"+cheese);\n    }\n}\n</code></pre>\n<p>Then, we create the Builder class — <strong><em>Cook,</em></strong> which declares an interface consisting of <em>all the steps of construction</em> involved.&nbsp;</p>\n<pre><code>public static interface Cook {\n    public void builddough();\n    public void buildbase();\n    public void buildtoppings();\n    public void buildsauce();\n    public void buildbake();\n    public void buildcheese();\n    \n    public Pizza GetPizza();    \n}\n</code></pre>\n<p>A major point to focus on is, <em>whether this interface will declare a method for returning the instance of the Pizza class or its concrete subclass will do it on its own?</em> The answer <strong>depends on the final representations</strong> of the complex product. Ideally, all the possible final representations (that is, all types of pizzas) will belong to the same Pizza class. But if final products differ a lot then there is no point in representing them with the same base class/interface.&nbsp;</p>\n<p>Therefore, <em>if each representation of the final product belongs to the same base class then we can declare the method for retrieving the final product in the Builder interface otherwise we have to do it individually in the concrete builder classes.</em></p>\n<p>Now, we subclass the <em>Cook</em> interface with different Cooks (e.g, <strong><em>MexicanCook and ItalianCook</em></strong>). These concrete builders provide implementations of the construction steps declared in <em>Cook</em> and will return the final product after applying all the steps.</p>\n<p><strong>ItalianCook Class</strong></p>\n<pre><code>public static class ItalianCook implements Cook {\n    private Pizza pizza;\n    public ItalianCook() {\n        this.pizza = new Pizza(); \n    }\n    @Override\n    public void builddough() {\n        pizza.setdough(\"Italian Dough\");\n    }\n    @Override\n    public void buildbase() {\n        pizza.setbase(\"Italian Base\");\n    }\n    @Override\n    public void buildtoppings() {\n        pizza.settoppings(\"Italian Toppings\");\n    }\n    @Override\n    public void buildsauce() {\n        pizza.setsauce(\"Italian Sauce\");\n    }\n    @Override\n    public void buildbake() {\n        pizza.setbake(\"Bake\");\n    }\n    @Override\n    public void buildcheese() {\n        pizza.setcheese(\"Cheese\");\n    }\n        \n    @Override\n    public Pizza GetPizza() {\n        Pizza final_pizza = this.pizza;\n        this.pizza = new Pizza();\n        return final_pizza;\n    }\n}\n</code></pre>\n<p><strong>MexicanCook Class</strong></p>\n<pre><code>public static class MexicanCook implements Cook {\n    private Pizza pizza;\n    public MexicanCook() {\n        this.pizza = new Pizza(); \n    }\n    @Override\n    public void builddough() {\n        pizza.setdough(\"Mexican Dough\");\n    }\n    @Override\n    public void buildbase() {\n        pizza.setbase(\"Mexican Base\");\n    }\n    @Override\n    public void buildtoppings() {\n        pizza.settoppings(\"Mexican Toppings\");\n    }\n    @Override\n    public void buildsauce() {\n        pizza.setsauce(\"Mexican Sauce\");\n    }\n    @Override\n    public void buildbake() {\n        pizza.setbake(\"Bake\");\n    }\n    @Override\n    public void buildcheese() {\n        pizza.setcheese(\"Cheese\");\n    }\n        \n    @Override\n    public Pizza GetPizza() {\n        Pizza final_pizza = this.pizza;\n        this.pizza = new Pizza();\n        return final_pizza;\n    }\n}\n</code></pre>\n<p><strong>NOTE:</strong> In the GetPizza() we’ve set the pizza field as a fresh Pizza so that whenever a pizza is completed a new pizza can be made in the future using the same object of Cook class (technically, the same object of a concrete subclass of Cook).</p>\n<p>Now, we declare the director-class — <strong>HeadChef</strong> that is responsible for defining the order in which steps are executed. There can be more than one director or a single director can itself have several construction processes. Here we’ve considered a single algorithm and a single Director.</p>\n<pre><code>public static class HeadChef {\n    private Cook cook;\n    public HeadChef (Cook cook) {\n        this.cook = cook;\n    }\n    public void MakePizza() {\n        cook.builddough();\n        cook.buildbase();\n        cook.buildtoppings();\n        cook.buildsauce();\n        cook.buildbake();\n        cook.buildcheese();\n    }\n}\n</code></pre>\n<p>Finally, we create the <strong>Consumer code</strong> which will create a concrete builder object and passes it to the director (HeadChef). Then, the director will apply the construction process with this builder object and finally, the complex product is retrieved either via the director or through the builder itself.&nbsp;<br>\nRetrieving the final product from the builder is also possible because the Client usually configures the director with the proper concrete builder, that is, the client knows which concrete builder will give him the product.</p>\n<pre><code>public static void main (String[] args){\n    Cook cook = new ItalianCook();\n    HeadChef headchef = new HeadChef (cook);\n       \n    headchef.MakePizza();\n    Pizza pizza = cook.GetPizza();\n    pizza.ShowPizza();\n    cook = new MexicanCook();\n    headchef = new HeadChef (cook);\n    headchef.MakePizza();\n    pizza = cook.GetPizza();\n    pizza.ShowPizza();\n}\n</code></pre>\n<h4>Implementation code&nbsp;C++</h4>\n<p><strong>Product class Pizza</strong></p>\n<pre><code>#include&lt;bits/stdc++.h&gt;\nusing namespace std;\nclass Pizza {\n    private:\n    string dough;\n    string base;\n    string toppings;\n    string sauce;\n    string bake;\n    string cheese;\npublic:\n    void setdough (string dough) {\n        this-&gt;dough = dough;\n    }\n    void setbase (string base) {\n        this-&gt;base = base;\n    }\n    void settoppings (string toppings) {\n        this-&gt;toppings = toppings;\n    }\n    void setsauce (string sauce) {\n        this-&gt;sauce = sauce;\n    }\n    void setbake (string bake) {\n        this-&gt;bake = bake;\n    }\n    void setcheese (string cheese) {\n        this-&gt;cheese = cheese;\n    }\n    void ShowPizza () {\n        cout&lt;&lt;dough&lt;&lt;\", \"&lt;&lt;base&lt;&lt;\", \"&lt;&lt;toppings&lt;&lt;\", \n                     \"&lt;&lt;sauce&lt;&lt;\", \"&lt;&lt;bake&lt;&lt;\", \"&lt;&lt;cheese&lt;&lt;endl;\n    }\n};\n</code></pre>\n<p>Builder interface Cook: the interface to declare steps of construction that is, methods that construct various parts of the product.</p>\n<pre><code>class Cook {\n    public:\n    virtual void builddough(){};\n    virtual void buildbase(){};\n    virtual void buildtoppings(){};\n    virtual void buildsauce(){};\n    virtual void buildbake(){};\n    virtual void buildcheese(){};\n    virtual Pizza* GetPizza (){};\n};\n</code></pre>\n<p><strong>Concrete Builder: ItalianCook</strong></p>\n<pre><code>class ItalianCook : public Cook {\n    //pizza is the final product this class will work on\n    private:\n    Pizza* pizza;\n    \n    public:\n    ItalianCook() {\n        this-&gt;pizza = new Pizza();\n    }\n    \n    void builddough() override {\n        pizza-&gt;setdough(\"Italian Dough\");\n    }\n    void buildbase() override {\n        pizza-&gt;setbase(\"Italian Base\");\n    }\n    void buildtoppings() override {\n        pizza-&gt;settoppings(\"Italian Toppings\");\n    }\n    void buildsauce() override {\n        pizza-&gt;setsauce(\"Italian Sauce\");\n    }\n    void buildbake() override {\n        pizza-&gt;setbake(\"Bake\");\n    }\n    void buildcheese() override {\n        pizza-&gt;setcheese(\"Cheese\");\n    }\n    Pizza* GetPizza() override {\n        Pizza* final_pizza = this-&gt;pizza;\n        this-&gt;pizza = new Pizza();\n        return final_pizza;\n    }\n};\n</code></pre>\n<p><strong>Concrete Builder: MexicanCook</strong></p>\n<pre><code>class MexicanCook : public Cook {\n    //pizza is the final product this class will work on\n    private:\n    Pizza* pizza;\n    public:\n   \n    MexicanCook() {\n        this-&gt;pizza = new Pizza();\n    }\n    //implementing the steps\n    void builddough() override {\n        pizza-&gt;setdough(\"Mexican Dough\");\n    }\n    void buildbase() override {\n        pizza-&gt;setbase(\"Mexican Base\");\n    }\n    void buildtoppings() override {\n        pizza-&gt;settoppings(\"Mexican Toppings\");\n    }\n    void buildsauce() override {\n        pizza-&gt;setsauce(\"Mexican Sauce\");\n    }\n    void buildbake() override {\n        pizza-&gt;setbake(\"Bake\");\n    }\n    void buildcheese() override {\n        pizza-&gt;setcheese(\"Cheese\");\n    }\n    Pizza* GetPizza() override {\n        Pizza* final_pizza = this-&gt;pizza;\n        this-&gt;pizza = new Pizza();\n        return final_pizza;        \n    }\n};\n</code></pre>\n<p><strong>Director Class HeadChef:</strong> responsible for defining the order in which steps are executed. There can be more than one director or a single director can&nbsp;itself have several construction processes. Here we’ve considered a single algorithm and a single Director.</p>\n<pre><code>class HeadChef {\n    private:\n    Cook* cook;\n    //cook is the concrete builder with which director works\n    public:\n    HeadChef (Cook* cook) {\n        this-&gt;cook = cook;\n    }\n    //using a HeadCHef without a cook is illogical therfore\n    //we define a parameterized constructor\n    void MakePizza() {\n        cook-&gt;builddough();\n        cook-&gt;buildbase();\n        cook-&gt;buildtoppings();\n        cook-&gt;buildsauce();\n        cook-&gt;buildbake();\n        cook-&gt;buildcheese();\n    }\n};\n</code></pre>\n<p>In this code, the main() acts as the Consumer code.</p>\n<pre><code>int main() {\n    Cook* supercook = new ItalianCook();\n    \n    HeadChef* headchef = new HeadChef (cook);\n    \n    headchef-&gt;MakePizza();\n    \n    Pizza* pizza = cook-&gt;GetPizza();\n    pizza-&gt;ShowPizza();\n    return 0;\n}\n</code></pre>\n<h3>When to apply Builder&nbsp;Pattern</h3>\n<p>We can use the builder pattern when</p>\n<ul>\n<li>Using a <strong><em>telescopic constructor</em></strong> is <strong>not efficient.</strong></li>\n<li>The <strong>same construction process</strong> is to be used to create <strong>different representations</strong> of the complex product.</li>\n<li>The <strong>process of creation of the product should be independent</strong> of the parts that make up the object and how they’re assembled.<br>\n<br>\nIn our example of Pizza making, the process of creation is defined by the <em>HeadChef</em> class whereas the parts that make up the object are implemented in the concrete builders that is, in <em>ItalianCook</em> and <em>MexicanCook</em>. It can be seen that the <strong>Builder class (<em>Cook</em>) hides the internal representation</strong> of the <em>Pizza</em> product that is, it hides the classes that define various parts of pizza and how these parts are assembled to complete the final product. Therefore, the process defined by <em>HeadChef</em> class is independent of the parts of the pizza product.</li>\n<li><strong>Adding new representations</strong> and removing existing representations of the complex product is frequent and needs to be flexible.</li>\n</ul>\n<h3>Consequences</h3>\n<ul>\n<li>It makes modifying representations easy and flexible. As we’ve already seen, the <strong>builder object interacts with the director via an abstract interface.</strong> Also, each builder implements that interface in a particular manner to provide a specific representation of the product. Because, the product is constructed through an interface, therefore, to add new representations, we’ve to add a new kind of concrete builder that implements the interface in a specific manner.</li>\n<li>It <strong>reduces the responsibility on the consumer</strong> (client) code. The consumer only needs to associate the correct builder class with the appropriate director-class (remember that there may exist more than one director).</li>\n<li>It allows us to use the <strong>same construction process</strong> to build different representations of the complex product.</li>\n<li>Additionally, the <strong>builder pattern gives finer control over the construction</strong> process. As compared to the <em>factory method or abstract factory method</em> who creates the objects in a single function call (that is, in one go!), the Builder pattern follows the step-by-step approach as directed by the director. The Builder interface depicts that, the final product is created by creating its various parts in a step-by-step manner. This gives us finer control over the construction process.</li>\n<li>It makes the application flexible at the cost of <strong>increases in the complexity and number of classes i</strong>n the code.</li>\n</ul>\n<h3>Used in</h3>\n<p>Some popular uses of the builder pattern can be found</p>\n<ul>\n<li>In core Java libraries such as<br>\n—<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/StringBuffer.html#append-boolean-\"> java.lang.StringBuffer#append()</a><br>\n— <a href=\"https://docs.oracle.com/javase/8/docs/api/java/nio/ByteBuffer.html#put-byte-\">java.nio.ByteBuffer#put()</a></li>\n<li>RTF converter application of ET++.<br>\nIt uses different types of product classes (as we’ve mentioned earlier). One can refer to a much more detailed explanation in the GoF book.</li>\n</ul>\n<h3>Conclusion</h3>\n<p>We saw some possible but inefficient approaches such as construction involving telescopic constructor and construction using setter methods via consumer code. Then, we saw the builder pattern and explored how this pattern solves the problems involved in previous approaches. For e.g, highly responsible consumer code, usage of the same process in the correct order, etc. Finally, we saw some of its applicability and its effect on the entire creational process.</p>\n<p><strong>Enjoy learning, Enjoy OOPS!</strong></p></div>",
        "createdAt": "2022-02-24T07:45:46.825Z",
        "updatedAt": "2022-02-26T19:27:02.372Z",
        "publishedAt": "2022-02-24T07:46:12.600Z"
        },
        {
            "description": "Given the root of a Binary Search Tree (BST), write a program to return the absolute minimum difference between the values of any two nodes in the tree.",
            "heading": "Minimum Absolute Difference in BST",
            "Body": "<div class=\"mt-6 ml-0 max-w-none md:ml-10 prose sm:prose-lg xl:prose-xl select-none\"><p><strong>Difficulty:</strong> Medium, <strong>Asked-in:</strong> Google</p>\n<p><strong>Key takeaway:</strong> An excellent problem to learn problem-solving using in-order traversal.</p>\n<h3>Let’s understand the&nbsp;problem!</h3>\n<p>Given the root of a binary search tree (BST), write a program to return the absolute minimum difference between the values of any two nodes in the tree.</p>\n<ul>\n<li>The node values in the tree can be positive, negative, or zero.</li>\n<li>Assume all node values in the tree are unique.</li>\n</ul>\n<p><strong>Examples</strong></p>\n<pre><code>Input: \n          10 \n        /   \\ \n      -2     13 \n      / \\   / \\ \n    -4   7 11  18\n\nOutput: 2\nExplanation: Among all pair of nodes, the absolute difference \nbetween pair of nodes (-2, -4) and (13, 11) is 2, which is \nminimum. So we return 2 as an output.\n\nInput:\n          10 \n        /   \\ \n       5     13 \n               \\ \n               18\nOutput: 3\nExplanation: Difference between pair of nodes (10, 13) is minimum. \n</code></pre>\n<p><strong>Important note:</strong>&nbsp;before moving on to the solutions, we recommend trying this problem on paper for atleast 15 or 30 minutes. Enjoy problem-solving!</p>\n<h3><strong>Discussed solution approaches</strong></h3>\n<ul>\n<li>A brute force approach by traversing the tree twice</li>\n<li>Using extra memory and recursive inorder traversal</li>\n<li>In-place solution using recursive inorder traversal</li>\n<li>Using iterative inorder traversal: stack-based approach</li>\n</ul>\n<h3>A brute force approach by traversing the tree twice</h3>\n<p>If a tree has n nodes, total possible pair of nodes = nC2 = n(n - 1)/2. So one idea would be to explore all pairs, find absolute differences among them, and track the minimum difference so far. For this, we need to traverse the tree twice: One traversal for accessing each node and another traversal for finding the absolute difference with all other nodes. In other words, we need to traverse the tree once for each node. </p>\n<p>Time complexity = n * O(n) = O(n^2). The critical question is: can we solve this problem using one traversal only? How can we think to solve this problem efficiently using the BST property? Let's think!</p>\n<h3>Using extra memory and recursive inorder traversal</h3>\n<p><strong>Solution idea</strong></p>\n<p>As we know, inorder traversal of BST traverse nodes in sorted order. If we store node values in extra memory of size n, then the problem gets transformed to find the min absolute difference in a sorted array. </p>\n<p>If we observe the sorted array, the difference between each element with its adjacent element is minimal compared to all other elements. So rather than exploring all pairs, we need to find the difference between n - 1 pair of adjacent elements and track the min absolute difference so far. We can do this in a single scan using O(n) time. Think!</p>\n<p><strong>Solution pseudocode</strong></p>\n<pre><code>int absMinDiffBST(BSTNode root)\n{\n    if (root == NULL)\n        return INT_MAX\n    vector&lt;int&gt; sorted\n    inorder(root, sorted)\n    int n = sorted.size()\n    int minDiff = INT_MAX\n    for (int i = 1; i &lt; n; i = i + 1)\n    {\n        int diff = sorted[i] - sorted[i - 1]\n        if (diff &lt; minDiff)\n            minDiff = diff\n    }\n    return minDiff\n}\n\nvoid inorder(BSTNode root, vector&lt;int&gt;&amp; sorted)\n{\n    if (root == NULL)\n        return\n    inorder(root-&gt;left, sorted)\n    sorted.push_back(root-&gt;data)\n    inorder(root-&gt;right, sorted)\n}\n</code></pre>\n<p><strong>Solution analysis</strong></p>\n<p>Time complexity = Time complexity of in-order traversal + Time complexity of finding min absolute difference in the sorted array = O(n) + O(n) = O(n)</p>\n<p>Space complexity = Space complexity of using an extra array of size n + Space complexity of recursive in-order traversal = O(n) + O(h) = O(n)</p>\n<p><strong>Note:</strong>&nbsp;the space complexity of recursive in-order traversal depends on the tree's height. In other words, we will only have as many frames on the call stack as the height of the tree. O(logn) in the best case (if the tree is entirely balanced) and O(n) in the worst-case (if we have a purely left-skewed or right-skewed tree).&nbsp;</p>\n<h3>In-place solution using recursive inorder traversal</h3>\n<p><strong>Solution idea</strong></p>\n<p>The critical questions are: can we solve this problem in place or without using extra space? Can we track the absolute minimum difference using inorder traversal? Let's think! There are two critical insights from the above approach:</p>\n<ul>\n<li>In-order traversal explores BST nodes in sorted order.</li>\n<li>In a sorted order, we need to compare the difference between the adjacent values to find the min absolute difference.</li>\n</ul>\n<p>So one idea would be to find the difference between the current node and the previous node during the in-order traversal itself. For this, we can maintain an extra pointer to track the previous node.</p>\n<ul>\n<li>We traverse the tree using in-order traversal. </li>\n<li>For every node, we calculate the difference between the previous node.</li>\n<li>If the difference is smaller than the minimum difference so far, we update the minimum difference.</li>\n</ul>\n<p><strong>Solution pseudocode</strong></p>\n<pre><code>int absMinDiffBST(BSTNode root)\n{\n    int minDiff = INT_MAX\n    BSTNode prev = NULL \n    minDiffBST(root, prev, &amp;minDiff)\n    return minDiff\n}\n\nvoid minDiffBST(BSTNode root, BSTNode prev, int&amp; minDiff)\n{\n    if (root == NULL) \n        return\n    minDiffBST(root-&gt;left, prev, minDiff)\n    if (prev != NULL)\n        minDiff = min(minDiff, root-&gt;data - prev-&gt;data)\n    prev = root\n    minDiffBST(root-&gt;right, prev, minDiff)\n}\n</code></pre>\n<p><strong>Solution analysis</strong></p>\n<p>We are performing O(1) operation with each node at each step of in-order traversal. So time complexity = Time complexity of in-order traversal = O(n). Space complexity = Space complexity of recursive in-order traversal = O(h)</p>\n<h3>Using iterative inorder traversal</h3>\n<p><strong>Solution idea</strong></p>\n<p>This idea is similar to the above approach, but instead of using recursive inorder traversal, we are using iterative traversal with the help of a stack. The idea is: when we pop the current node from the stack, we compare the minimum difference found so far with the difference between the previous node and the current node. If this difference is smaller than the minimum difference so far, we update the minimum difference.</p>\n<p><strong>Solution pseudocode</strong></p>\n<pre><code>int absMinDiffBST(BSTNode root)\n{\n    int minDiff = INT_MAX\n    Stack&lt;BSTNode&gt; stack\n    BSTNode curr = root\n    prev = NULL\n    while (stack.empty() == false || curr != Null) \n    {\n        if (curr != NULL) \n        {\n            stack.push(curr)\n            curr = cur-&gt;left\n        } \n        else \n        {\n            curr = stack.pop()\n            if (prev != NULL) \n                minDiff = min(minDiff, curr-&gt;data - prev-&gt;data)\n            prev = curr\n            curr = curr-&gt;right\n        }\n    }\n    return minDiff\n}\n</code></pre>\n<p><strong>Solution analysis</strong></p>\n<p>We are traversing each node only once and performing O(1) operation with each node. So time complexity = Time complexity of in-order traversal = O(n). Space complexity = Space complexity of using stack for in-order traversal = O(h). Think!</p>\n<p><strong>Important note:</strong>&nbsp;we recommend transforming the above pseudo-codes into a favorite programming language (C, C++, Java, Python, etc.) and verifying all the test cases. Enjoy programming!</p>\n<h3><strong>Critical ideas to think!</strong></h3>\n<ul>\n<li>How do we solve this problem if a tree is not BST? Can we think of using the above approaches?</li>\n<li>In the above approaches, why are we not taking the absolute value of (root-&gt;data - prev-&gt;data)? Is there a possibility that (root-&gt;data - prev-&gt;data) can be negative?</li>\n<li>Can we solve this problem using other traversals like pre-order, post-order, or level order?</li>\n<li>Can we solve this problem using threaded binary tree traversal or morris traversal?</li>\n<li>Why are we passing the minDiff parameter as a reference in the function call in the second approach? Can we think of some different ways to implement the code?</li>\n<li>Does the above solution work perfectly when values are repeated in the BST?</li>\n</ul>\n<h3>Comparison of time and space complexities</h3>\n<ul>\n<li>By traversing the tree twice: Time = O(n^2), Space = O(h)</li>\n<li>Inorder and extra space: Time = O(n), Space = O(n)</li>\n<li>Inorder without extra space: Time = O(n), Space = O(h)</li>\n<li>Iterative inorder using stack: Time = O(n), Space = O(h)</li>\n</ul>\n<h3>Suggested coding problems to&nbsp;practice</h3>\n<ul>\n<li><a href=\"https://leetcode.com/problems/binary-tree-inorder-traversal/\">Recursive and iterative in-order traversal</a></li>\n<li><a href=\"https://leetcode.com/problems/k-diff-pairs-in-an-array/\">K-diff Pairs in an Array</a></li>\n<li>Find the minimum absolute difference in two different BST’s</li>\n<li><a href=\"https://leetcode.com/problems/kth-smallest-element-in-a-bst/\">Kth Smallest Element in a BST</a></li>\n</ul>\n<p>Please write in the message below if you find anything incorrect, or you want to share more insight. Enjoy learning, Enjoy algorithms!</p></div>",
        "createdAt": "2022-02-24T07:45:46.825Z",
        "updatedAt": "2022-02-26T19:27:02.372Z",
        "publishedAt": "2022-02-24T07:46:12.600Z"
        },
        {
            "description": "",
            "heading": "",
            "Body": "",
        "createdAt": "2022-02-24T07:45:46.825Z",
        "updatedAt": "2022-02-26T19:27:02.372Z",
        "publishedAt": "2022-02-24T07:46:12.600Z"
        }
    ]
}
